{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on DeepPavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**:\n",
    "Intent recognition on SNIPS dataset: https://github.com/snipsco/nlu-benchmark/tree/master/2017-06-custom-intent-engines that has already been recomposed to `csv` format and can be downloaded from http://files.deeppavlov.ai/datasets/snips_intents/train.csv\n",
    "\n",
    "FastText English word embeddings ~8Gb: http://files.deeppavlov.ai/deeppavlov_data/embeddings/wiki.en.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan of the notebook:\n",
    "\n",
    "1. [Data aggregation](#Data-aggregation)\n",
    "     * [DatasetReader](#DatasetReader)\n",
    "     * [DatasetIterator](#DatasetIterator)\n",
    "2. [Data preprocessing](#Data-preprocessing)\n",
    "     * [Lowercasing](#Lowercasing)\n",
    "     * [Tokenization](#Tokenization)\n",
    "     * [Vocabulary](#Vocabulary)\n",
    "3. [Featurization](#Featurization)\n",
    "    * [Bag-of-words embedder](#Bag-of-words)\n",
    "    * [TF-IDF vectorizer](#TF-IDF Vectorizer)\n",
    "    * [fastText embedder](#fastText-embedder)\n",
    "    * [fastText weighted by TF-IDF embedder](#fastText-weighted-by-TF-IDF-embedder)\n",
    "    * [Mean fastText embedder](#Mean-fastText-embedder)\n",
    "4. [Models](#Models)\n",
    "    * [Building models in python](#Models-in-python)\n",
    "        - [Sklearn component classifiers](#SklearnComponent-classifier-on-Tfidf-features-in-python)\n",
    "        - [Keras classification models on fastText emb](#KerasClassificationModel-on-fastText-embeddings-in-python)\n",
    "        - [Keras classification models on fastText weighted emb](#KerasClassificationModel-on-fastText-weighted-by-TF-IDF-embeddings-in-python)\n",
    "    * [Building models from configs](#Models-from-configs)\n",
    "        - [Sklearn component classifiers](#SklearnComponent-classifier-on-Tfidf-features-from-config)\n",
    "        - [Keras classification models](#KerasClassificationModel-on-fastText-embeddings-from-config)\n",
    "        - [Keras classification models on fastText weighted emb](#KerasClassificationModel-on-fastText-weighted-by-TF-IDF-embeddings-from-config)\n",
    "    * [Bonus: pre-trained CNN model in DeepPavlov](#Bonus:-pre-trained-CNN-model-in-DeepPavlov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's download and look into data we will work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:32:05.2 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 208: Starting new HTTP connection (1): files.deeppavlov.ai\n",
      "2018-10-29 16:32:05.70 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 396: http://files.deeppavlov.ai:80 \"GET /datasets/snips_intents/train.csv HTTP/1.1\" 200 980824\n",
      "2018-10-29 16:32:05.72 INFO in 'deeppavlov.core.data.utils'['utils'] at line 59: Downloading from http://files.deeppavlov.ai/datasets/snips_intents/train.csv to snips/train.csv\n",
      "100%|██████████| 981k/981k [00:00<00:00, 22.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.core.data.utils import simple_download\n",
    "from deeppavlov.core.commands.utils import set_deeppavlov_root\n",
    "\n",
    "# assign root for all the file paths to current directory\n",
    "set_deeppavlov_root(config={\"deeppavlov_root\": \".\"})\n",
    "\n",
    "#download train data file for SNIPS\n",
    "simple_download(url=\"http://files.deeppavlov.ai/datasets/snips_intents/train.csv\", \n",
    "                destination=\"./snips/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text,intents\r\n",
      "Add another song to the Cita RomГЎntica playlist. ,AddToPlaylist\r\n",
      "add clem burke in my playlist Pre-Party R&B Jams,AddToPlaylist\r\n",
      "Add Live from Aragon Ballroom to Trapeo,AddToPlaylist\r\n",
      "add Unite and Win to my night out,AddToPlaylist\r\n",
      "Add track to my Digster Future Hits,AddToPlaylist\r\n",
      "add the piano bar to my Cindy Wilson,AddToPlaylist\r\n",
      "Add Spanish Harlem Incident to cleaning the house,AddToPlaylist\r\n",
      "add The Greyest of Blue Skies in Indie EspaГ±ol my playlist,AddToPlaylist\r\n",
      "Add the name kids in the street to the plylist New Indie Mix,AddToPlaylist\r\n",
      "add album radar latino,AddToPlaylist\r\n",
      "Add Tranquility to the Latin Pop Rising playlist. ,AddToPlaylist\r\n",
      "Add d flame to the Dcode2016 playlist.,AddToPlaylist\r\n",
      "Add album to my fairy tales,AddToPlaylist\r\n",
      "I need another artist in the New Indie Mix playlist. ,AddToPlaylist\r\n"
     ]
    }
   ],
   "source": [
    "! head -n 15 snips/train.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatasetReader\n",
    "\n",
    "Read data using `BasicClassificationDatasetReader` из DeepPavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:13:12.657 DEBUG in 'matplotlib'['__init__'] at line 415: $HOME=/home/dilyara\n",
      "2018-10-29 15:13:12.658 DEBUG in 'matplotlib'['__init__'] at line 415: CONFIGDIR=/home/dilyara/.config/matplotlib\n",
      "2018-10-29 15:13:12.659 DEBUG in 'matplotlib'['__init__'] at line 415: matplotlib data path: /home/dilyara/anaconda3/envs/deep36_reserve/lib/python3.6/site-packages/matplotlib/mpl-data\n",
      "2018-10-29 15:13:12.662 DEBUG in 'matplotlib'['__init__'] at line 1085: loaded rc file /home/dilyara/anaconda3/envs/deep36_reserve/lib/python3.6/site-packages/matplotlib/mpl-data/matplotlibrc\n",
      "2018-10-29 15:13:12.664 DEBUG in 'matplotlib'['__init__'] at line 1794: matplotlib version 3.0.0\n",
      "2018-10-29 15:13:12.664 DEBUG in 'matplotlib'['__init__'] at line 1795: interactive is False\n",
      "2018-10-29 15:13:12.665 DEBUG in 'matplotlib'['__init__'] at line 1796: platform is linux\n",
      "2018-10-29 15:13:12.665 DEBUG in 'matplotlib'['__init__'] at line 1797: loaded modules: ['builtins', 'sys', '_frozen_importlib', '_imp', '_warnings', '_thread', '_weakref', '_frozen_importlib_external', '_io', 'marshal', 'posix', 'zipimport', 'encodings', 'codecs', '_codecs', 'encodings.aliases', 'encodings.utf_8', '_signal', '__main__', 'encodings.latin_1', 'io', 'abc', '_weakrefset', '_bootlocale', '_locale', 'site', 'os', 'errno', 'stat', '_stat', 'posixpath', 'genericpath', 'os.path', '_collections_abc', '_sitebuiltins', 'sysconfig', '_sysconfigdata_m_linux_x86_64-linux-gnu', 'types', 'functools', '_functools', 'collections', 'operator', '_operator', 'keyword', 'heapq', '_heapq', 'itertools', 'reprlib', '_collections', 'weakref', 'collections.abc', 'importlib', 'importlib._bootstrap', 'importlib._bootstrap_external', 'warnings', 'importlib.util', 'importlib.abc', 'importlib.machinery', 'contextlib', 'mpl_toolkits', 'google', 'sphinxcontrib', 'encodings.cp437', 'runpy', 'pkgutil', 'ipykernel', 'ipykernel._version', 'ipykernel.connect', '__future__', 'json', 'json.decoder', 're', 'enum', 'sre_compile', '_sre', 'sre_parse', 'sre_constants', 'copyreg', 'json.scanner', '_json', 'json.encoder', 'subprocess', 'time', 'signal', '_posixsubprocess', 'select', 'selectors', 'math', 'threading', 'traceback', 'linecache', 'tokenize', 'token', 'IPython', 'IPython.core', 'IPython.core.getipython', 'IPython.core.release', 'IPython.core.application', 'atexit', 'copy', 'glob', 'fnmatch', 'logging', 'string', '_string', 'shutil', 'zlib', 'bz2', '_compression', '_bz2', 'lzma', '_lzma', 'pwd', 'grp', 'traitlets', 'traitlets.traitlets', 'inspect', 'ast', '_ast', 'dis', 'opcode', '_opcode', 'six', 'struct', '_struct', 'traitlets.utils', 'traitlets.utils.getargspec', 'traitlets.utils.importstring', 'ipython_genutils', 'ipython_genutils._version', 'ipython_genutils.py3compat', 'ipython_genutils.encoding', 'locale', 'platform', 'traitlets.utils.sentinel', 'traitlets.utils.bunch', 'traitlets._version', 'traitlets.config', 'traitlets.config.application', 'decorator', 'traitlets.config.configurable', 'traitlets.config.loader', 'argparse', 'textwrap', 'gettext', 'ipython_genutils.path', 'random', 'hashlib', '_hashlib', '_blake2', '_sha3', 'bisect', '_bisect', '_random', 'ipython_genutils.text', 'ipython_genutils.importstring', 'IPython.core.crashhandler', 'pprint', 'IPython.core.ultratb', 'pydoc', 'urllib', 'urllib.parse', 'IPython.core.debugger', 'bdb', 'IPython.utils', 'IPython.utils.PyColorize', 'IPython.utils.coloransi', 'IPython.utils.ipstruct', 'IPython.utils.colorable', 'pygments', 'pygments.util', 'IPython.utils.py3compat', 'IPython.utils.encoding', 'IPython.core.excolors', 'IPython.testing', 'IPython.testing.skipdoctest', 'pdb', 'cmd', 'code', 'codeop', 'IPython.core.display_trap', 'IPython.utils.path', 'IPython.utils.process', 'IPython.utils._process_posix', 'pexpect', 'pexpect.exceptions', 'pexpect.utils', 'pexpect.expect', 'pexpect.pty_spawn', 'pty', 'tty', 'termios', 'ptyprocess', 'ptyprocess.ptyprocess', 'fcntl', 'resource', 'ptyprocess.util', 'pexpect.spawnbase', 'pexpect.run', 'IPython.utils._process_common', 'shlex', 'IPython.utils.decorators', 'IPython.utils.data', 'IPython.utils.terminal', 'IPython.utils.sysinfo', 'IPython.utils._sysinfo', 'IPython.core.profiledir', 'IPython.paths', 'tempfile', 'IPython.utils.importstring', 'IPython.terminal', 'IPython.terminal.embed', 'IPython.core.compilerop', 'IPython.core.magic_arguments', 'IPython.core.error', 'IPython.utils.text', 'pathlib', 'ntpath', 'IPython.core.magic', 'getopt', 'IPython.core.oinspect', 'IPython.core.page', 'IPython.core.display', 'binascii', 'mimetypes', 'IPython.lib', 'IPython.lib.security', 'getpass', 'IPython.lib.pretty', 'datetime', '_datetime', 'IPython.utils.openpy', 'IPython.utils.dir2', 'IPython.utils.wildcard', 'pygments.lexers', 'pygments.lexers._mapping', 'pygments.modeline', 'pygments.plugin', 'pygments.lexers.python', 'pygments.lexer', 'pygments.filter', 'pygments.filters', 'pygments.token', 'pygments.regexopt', 'pygments.unistring', 'pygments.formatters', 'pygments.formatters._mapping', 'pygments.formatters.html', 'pygments.formatter', 'pygments.styles', 'IPython.core.inputtransformer2', 'typing', 'typing.io', 'typing.re', 'IPython.core.interactiveshell', 'asyncio', 'asyncio.base_events', 'concurrent', 'concurrent.futures', 'concurrent.futures._base', 'concurrent.futures.process', 'queue', 'multiprocessing', 'multiprocessing.context', 'multiprocessing.process', 'multiprocessing.reduction', 'pickle', '_compat_pickle', '_pickle', 'socket', '_socket', 'array', '__mp_main__', 'multiprocessing.connection', '_multiprocessing', 'multiprocessing.util', 'concurrent.futures.thread', 'asyncio.compat', 'asyncio.coroutines', 'asyncio.constants', 'asyncio.events', 'asyncio.base_futures', 'asyncio.log', 'asyncio.futures', 'asyncio.base_tasks', '_asyncio', 'asyncio.tasks', 'asyncio.locks', 'asyncio.protocols', 'asyncio.queues', 'asyncio.streams', 'asyncio.subprocess', 'asyncio.transports', 'asyncio.unix_events', 'asyncio.base_subprocess', 'asyncio.selector_events', 'ssl', 'ipaddress', '_ssl', 'base64', 'asyncio.sslproto', 'pickleshare', 'IPython.core.prefilter', 'IPython.core.autocall', 'IPython.core.macro', 'IPython.core.splitinput', 'IPython.core.alias', 'IPython.core.builtin_trap', 'IPython.core.events', 'backcall', 'backcall.backcall', 'IPython.core.displayhook', 'IPython.core.displaypub', 'IPython.core.extensions', 'IPython.core.formatters', 'IPython.utils.sentinel', 'IPython.core.history', 'sqlite3', 'sqlite3.dbapi2', '_sqlite3', 'IPython.core.logger', 'IPython.core.payload', 'IPython.core.usage', 'IPython.display', 'IPython.lib.display', 'html', 'html.entities', 'IPython.utils.io', 'IPython.utils.capture', 'IPython.utils.strdispatch', 'IPython.core.hooks', 'IPython.utils.syspathcontext', 'IPython.utils.tempdir', 'IPython.utils.contexts', 'IPython.core.async_helpers', 'IPython.terminal.interactiveshell', 'prompt_toolkit', 'prompt_toolkit.application', 'prompt_toolkit.application.application', 'prompt_toolkit.buffer', 'prompt_toolkit.application.current', 'prompt_toolkit.eventloop', 'prompt_toolkit.eventloop.base', 'prompt_toolkit.log', 'prompt_toolkit.eventloop.coroutine', 'prompt_toolkit.eventloop.defaults', 'prompt_toolkit.utils', 'six.moves', 'wcwidth', 'wcwidth.wcwidth', 'wcwidth.table_wide', 'wcwidth.table_zero', 'prompt_toolkit.cache', 'prompt_toolkit.eventloop.future', 'prompt_toolkit.eventloop.context', 'prompt_toolkit.eventloop.async_generator', 'six.moves.queue', 'prompt_toolkit.eventloop.event', 'prompt_toolkit.application.run_in_terminal', 'prompt_toolkit.auto_suggest', 'prompt_toolkit.filters', 'prompt_toolkit.filters.base', 'prompt_toolkit.filters.app', 'prompt_toolkit.enums', 'prompt_toolkit.filters.utils', 'prompt_toolkit.filters.cli', 'prompt_toolkit.clipboard', 'prompt_toolkit.clipboard.base', 'prompt_toolkit.selection', 'prompt_toolkit.clipboard.in_memory', 'prompt_toolkit.completion', 'prompt_toolkit.completion.base', 'prompt_toolkit.completion.filesystem', 'prompt_toolkit.completion.word_completer', 'prompt_toolkit.document', 'prompt_toolkit.history', 'prompt_toolkit.search', 'prompt_toolkit.key_binding', 'prompt_toolkit.key_binding.key_bindings', 'prompt_toolkit.keys', 'prompt_toolkit.key_binding.vi_state', 'prompt_toolkit.validation', 'prompt_toolkit.input', 'prompt_toolkit.input.base', 'prompt_toolkit.input.defaults', 'prompt_toolkit.input.typeahead', 'prompt_toolkit.key_binding.bindings', 'prompt_toolkit.key_binding.bindings.page_navigation', 'prompt_toolkit.key_binding.bindings.scroll', 'prompt_toolkit.key_binding.defaults', 'prompt_toolkit.key_binding.bindings.basic', 'prompt_toolkit.key_binding.key_processor', 'prompt_toolkit.key_binding.bindings.named_commands', 'prompt_toolkit.key_binding.bindings.completion', 'prompt_toolkit.key_binding.bindings.emacs', 'prompt_toolkit.key_binding.bindings.vi', 'prompt_toolkit.input.vt100_parser', 'prompt_toolkit.input.ansi_escape_sequences', 'prompt_toolkit.key_binding.digraphs', 'prompt_toolkit.key_binding.bindings.mouse', 'prompt_toolkit.layout', 'prompt_toolkit.layout.containers', 'prompt_toolkit.layout.controls', 'prompt_toolkit.formatted_text', 'prompt_toolkit.formatted_text.base', 'prompt_toolkit.formatted_text.html', 'xml', 'xml.dom', 'xml.dom.domreg', 'xml.dom.minidom', 'xml.dom.minicompat', 'xml.dom.xmlbuilder', 'xml.dom.NodeFilter', 'prompt_toolkit.formatted_text.ansi', 'prompt_toolkit.output', 'prompt_toolkit.output.base', 'prompt_toolkit.layout.screen', 'prompt_toolkit.output.defaults', 'prompt_toolkit.output.color_depth', 'prompt_toolkit.output.vt100', 'prompt_toolkit.styles', 'prompt_toolkit.styles.base', 'prompt_toolkit.styles.defaults', 'prompt_toolkit.styles.style', 'prompt_toolkit.styles.named_colors', 'prompt_toolkit.styles.pygments', 'prompt_toolkit.styles.style_transformation', 'colorsys', 'prompt_toolkit.formatted_text.pygments', 'prompt_toolkit.formatted_text.utils', 'prompt_toolkit.lexers', 'prompt_toolkit.lexers.base', 'prompt_toolkit.lexers.pygments', 'prompt_toolkit.mouse_events', 'prompt_toolkit.layout.processors', 'prompt_toolkit.layout.utils', 'prompt_toolkit.layout.dimension', 'prompt_toolkit.layout.margins', 'prompt_toolkit.layout.layout', 'prompt_toolkit.layout.menus', 'prompt_toolkit.renderer', 'prompt_toolkit.layout.mouse_handlers', 'prompt_toolkit.key_binding.bindings.cpr', 'prompt_toolkit.key_binding.emacs_state', 'prompt_toolkit.layout.dummy', 'prompt_toolkit.application.dummy', 'prompt_toolkit.shortcuts', 'prompt_toolkit.shortcuts.dialogs', 'prompt_toolkit.key_binding.bindings.focus', 'prompt_toolkit.widgets', 'prompt_toolkit.widgets.base', 'prompt_toolkit.widgets.toolbars', 'prompt_toolkit.widgets.dialogs', 'prompt_toolkit.widgets.menus', 'prompt_toolkit.shortcuts.prompt', 'prompt_toolkit.key_binding.bindings.auto_suggest', 'prompt_toolkit.key_binding.bindings.open_in_editor', 'prompt_toolkit.shortcuts.utils', 'prompt_toolkit.shortcuts.progress_bar', 'prompt_toolkit.shortcuts.progress_bar.base', 'prompt_toolkit.shortcuts.progress_bar.formatters', 'prompt_toolkit.patch_stdout', 'pygments.style', 'IPython.terminal.debugger', 'IPython.core.completer', 'unicodedata', 'IPython.core.latex_symbols', 'IPython.utils.generics', 'simplegeneric', 'jedi', 'jedi.api', 'parso', 'parso.parser', 'parso.tree', 'parso._compatibility', 'parso.pgen2', 'parso.pgen2.generator', 'parso.pgen2.grammar_parser', 'parso.python', 'parso.python.tokenize', 'parso.python.token', 'parso.utils', 'parso.grammar', 'parso.python.diff', 'difflib', 'parso.python.parser', 'parso.python.tree', 'parso.python.prefix', 'parso.cache', 'gc', 'parso.python.errors', 'parso.normalizer', 'parso.python.pep8', 'jedi._compatibility', 'jedi.parser_utils', 'jedi.debug', 'jedi.settings', 'jedi.cache', 'jedi.api.classes', 'jedi.evaluate', 'jedi.evaluate.utils', 'jedi.evaluate.imports', 'jedi.evaluate.sys_path', 'jedi.evaluate.cache', 'jedi.evaluate.base_context', 'jedi.common', 'jedi.common.context', 'jedi.evaluate.helpers', 'jedi.common.utils', 'jedi.evaluate.compiled', 'jedi.evaluate.compiled.context', 'jedi.evaluate.filters', 'jedi.evaluate.flow_analysis', 'jedi.evaluate.recursion', 'jedi.evaluate.lazy_context', 'jedi.evaluate.compiled.access', 'jedi.evaluate.compiled.getattr_static', 'jedi.evaluate.compiled.fake', 'jedi.evaluate.analysis', 'jedi.evaluate.context', 'jedi.evaluate.context.module', 'jedi.evaluate.context.klass', 'jedi.evaluate.context.function', 'jedi.evaluate.docstrings', 'jedi.evaluate.pep0484', 'jedi.evaluate.arguments', 'jedi.evaluate.context.iterable', 'jedi.evaluate.param', 'jedi.evaluate.context.asynchronous', 'jedi.evaluate.parser_cache', 'jedi.evaluate.context.instance', 'jedi.evaluate.syntax_tree', 'jedi.evaluate.finder', 'jedi.api.keywords', 'pydoc_data', 'pydoc_data.topics', 'jedi.api.interpreter', 'jedi.evaluate.compiled.mixed', 'jedi.api.helpers', 'jedi.api.completion', 'jedi.api.environment', 'filecmp', 'jedi.evaluate.compiled.subprocess', 'jedi.evaluate.compiled.subprocess.functions', 'jedi.api.exceptions', 'jedi.api.project', 'jedi.evaluate.usages', 'IPython.terminal.ptutils', 'IPython.terminal.shortcuts', 'IPython.terminal.magics', 'IPython.lib.clipboard', 'IPython.terminal.pt_inputhooks', 'IPython.terminal.prompts', 'IPython.terminal.ipapp', 'IPython.core.magics', 'IPython.core.magics.auto', 'IPython.core.magics.basic', 'IPython.core.magics.code', 'urllib.request', 'email', 'http', 'http.client', 'email.parser', 'email.feedparser', 'email.errors', 'email._policybase', 'email.header', 'email.quoprimime', 'email.base64mime', 'email.charset', 'email.encoders', 'quopri', 'email.utils', 'email._parseaddr', 'calendar', 'email.message', 'uu', 'email._encoded_words', 'email.iterators', 'urllib.error', 'urllib.response', 'IPython.core.magics.config', 'IPython.core.magics.display', 'IPython.core.magics.execution', 'timeit', 'cProfile', '_lsprof', 'profile', 'optparse', 'pstats', 'IPython.utils.module_paths', 'IPython.utils.timing', 'IPython.core.magics.extension', 'IPython.core.magics.history', 'IPython.core.magics.logging', 'IPython.core.magics.namespace', 'IPython.core.magics.osm', 'IPython.core.magics.pylab', 'IPython.core.pylabtools', 'IPython.core.magics.script', 'IPython.lib.backgroundjobs', 'IPython.core.shellapp', 'IPython.extensions', 'IPython.extensions.storemagic', 'IPython.utils.frame', 'jupyter_client', 'jupyter_client._version', 'jupyter_client.connect', 'zmq', 'ctypes', '_ctypes', 'ctypes._endian', 'zmq.backend', 'zmq.backend.select', 'zmq.backend.cython', 'cython_runtime', 'zmq.backend.cython.constants', '_cython_0_28_5', 'zmq.backend.cython.error', 'zmq.backend.cython.message', 'zmq.error', 'zmq.backend.cython.context', 'zmq.backend.cython.socket', 'zmq.backend.cython.utils', 'zmq.backend.cython._poll', 'zmq.backend.cython._version', 'zmq.backend.cython._device', 'zmq.sugar', 'zmq.sugar.constants', 'zmq.utils', 'zmq.utils.constant_names', 'zmq.sugar.context', 'zmq.sugar.attrsettr', 'zmq.sugar.socket', 'zmq.sugar.poll', 'zmq.utils.jsonapi', 'zmq.utils.strtypes', 'zmq.sugar.frame', 'zmq.sugar.tracker', 'zmq.sugar.version', 'zmq.sugar.stopwatch', 'jupyter_client.localinterfaces', 'jupyter_core', 'jupyter_core.version', 'jupyter_core.paths', 'jupyter_client.launcher', 'traitlets.log', 'jupyter_client.client', 'jupyter_client.channels', 'jupyter_client.channelsabc', 'jupyter_client.clientabc', 'jupyter_client.manager', 'jupyter_client.kernelspec', 'jupyter_client.managerabc', 'jupyter_client.blocking', 'jupyter_client.blocking.client', 'jupyter_client.blocking.channels', 'jupyter_client.multikernelmanager', 'uuid', 'ctypes.util', 'ipykernel.kernelapp', 'tornado', 'tornado.ioloop', 'numbers', 'tornado.concurrent', 'tornado.log', 'logging.handlers', 'tornado.escape', 'tornado.util', 'tornado.speedups', 'curses', '_curses', 'tornado.stack_context', 'tornado.platform', 'tornado.platform.auto', 'tornado.platform.posix', 'tornado.platform.common', 'tornado.platform.interface', 'zmq.eventloop', 'zmq.eventloop.ioloop', 'tornado.platform.asyncio', 'tornado.gen', 'zmq.eventloop.zmqstream', 'ipykernel.iostream', 'imp', 'jupyter_client.session', 'hmac', 'jupyter_client.jsonutil', 'dateutil', 'dateutil._version', 'dateutil.parser', 'dateutil.parser._parser', 'decimal', '_decimal', 'dateutil.relativedelta', 'dateutil._common', 'dateutil.tz', 'dateutil.tz.tz', 'dateutil.tz._common', 'dateutil.tz._factories', 'dateutil.parser.isoparser', '_strptime', 'jupyter_client.adapter', 'ipykernel.heartbeat', 'ipykernel.ipkernel', 'IPython.utils.tokenutil', 'ipykernel.comm', 'ipykernel.comm.manager', 'ipykernel.comm.comm', 'ipykernel.kernelbase', 'ipykernel.jsonutil', 'ipykernel.zmqshell', 'IPython.core.payloadpage', 'ipykernel.displayhook', 'ipykernel.parentpoller', 'faulthandler', 'ipykernel.datapub', 'ipykernel.serialize', 'ipykernel.pickleutil', 'ipykernel.codeutil', 'IPython.core.completerlib', 'storemagic', 'deeppavlov', 'deeppavlov.core', 'deeppavlov.core.data', 'deeppavlov.core.data.utils', 'gzip', 'secrets', 'tarfile', 'zipfile', 'numpy', 'numpy._globals', 'numpy.__config__', 'numpy.version', 'numpy._import_tools', 'numpy.add_newdocs', 'numpy.lib', 'numpy.lib.info', 'numpy.lib.type_check', 'numpy.core', 'numpy.core.info', 'numpy.core.multiarray', 'numpy.core.umath', 'numpy.core._internal', 'numpy.compat', 'numpy.compat._inspect', 'numpy.compat.py3k', 'numpy.core.numerictypes', 'numpy.core.numeric', 'numpy.core.fromnumeric', 'numpy.core._methods', 'numpy.core.arrayprint', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core.machar', 'numpy.core.getlimits', 'numpy.core.shape_base', 'numpy.core.einsumfunc', 'numpy.testing', 'unittest', 'unittest.result', 'unittest.util', 'unittest.case', 'unittest.suite', 'unittest.loader', 'unittest.main', 'unittest.runner', 'unittest.signals', 'numpy.testing.decorators', 'numpy.testing.nose_tools', 'numpy.testing.nose_tools.decorators', 'numpy.testing.nose_tools.utils', 'numpy.lib.utils', 'numpy.testing.nosetester', 'numpy.testing.nose_tools.nosetester', 'numpy.testing.utils', 'numpy.lib.ufunclike', 'numpy.lib.index_tricks', 'numpy.lib.function_base', 'numpy.lib.twodim_base', 'numpy.matrixlib', 'numpy.matrixlib.defmatrix', 'numpy.lib.stride_tricks', 'numpy.lib.mixins', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.scimath', 'numpy.lib.polynomial', 'numpy.linalg', 'numpy.linalg.info', 'numpy.linalg.linalg', 'numpy.linalg.lapack_lite', 'numpy.linalg._umath_linalg', 'numpy.lib.arraysetops', 'numpy.lib.npyio', 'numpy.lib.format', 'numpy.lib._datasource', 'numpy.lib._iotools', 'numpy.lib.financial', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy._distributor_init', 'numpy.fft', 'numpy.fft.info', 'numpy.fft.fftpack', 'numpy.fft.fftpack_lite', 'numpy.fft.helper', 'numpy.polynomial', 'numpy.polynomial.polynomial', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.random', 'numpy.random.info', 'mtrand', 'numpy.random.mtrand', 'numpy.ctypeslib', 'numpy.ma', 'numpy.ma.core', 'numpy.ma.extras', 'requests', 'urllib3', 'urllib3.connectionpool', 'urllib3.exceptions', 'urllib3.packages', 'urllib3.packages.ssl_match_hostname', 'urllib3.packages.six', 'urllib3.packages.six.moves', 'urllib3.packages.six.moves.http_client', 'urllib3.connection', 'urllib3.util', 'urllib3.util.connection', 'urllib3.util.wait', 'urllib3.util.selectors', 'urllib3.util.request', 'urllib3.util.response', 'urllib3.util.ssl_', 'urllib3.util.timeout', 'urllib3.util.retry', 'urllib3.util.url', 'urllib3._collections', 'urllib3.request', 'urllib3.filepost', 'urllib3.fields', 'urllib3.packages.six.moves.urllib', 'urllib3.packages.six.moves.urllib.parse', 'urllib3.response', 'urllib3.poolmanager', 'chardet', 'chardet.compat', 'chardet.universaldetector', 'chardet.charsetgroupprober', 'chardet.enums', 'chardet.charsetprober', 'chardet.escprober', 'chardet.codingstatemachine', 'chardet.escsm', 'chardet.latin1prober', 'chardet.mbcsgroupprober', 'chardet.utf8prober', 'chardet.mbcssm', 'chardet.sjisprober', 'chardet.mbcharsetprober', 'chardet.chardistribution', 'chardet.euctwfreq', 'chardet.euckrfreq', 'chardet.gb2312freq', 'chardet.big5freq', 'chardet.jisfreq', 'chardet.jpcntx', 'chardet.eucjpprober', 'chardet.gb2312prober', 'chardet.euckrprober', 'chardet.cp949prober', 'chardet.big5prober', 'chardet.euctwprober', 'chardet.sbcsgroupprober', 'chardet.sbcharsetprober', 'chardet.langcyrillicmodel', 'chardet.langgreekmodel', 'chardet.langbulgarianmodel', 'chardet.langthaimodel', 'chardet.langhebrewmodel', 'chardet.hebrewprober', 'chardet.langturkishmodel', 'chardet.version', 'requests.exceptions', 'urllib3.contrib', 'requests.__version__', 'requests.utils', 'requests.certs', 'certifi', 'certifi.core', 'requests._internal_utils', 'requests.compat', 'http.cookiejar', 'http.cookies', 'requests.cookies', 'requests.structures', 'requests.packages', 'requests.packages.urllib3', 'requests.packages.urllib3.connectionpool', 'requests.packages.urllib3.exceptions', 'requests.packages.urllib3.packages', 'requests.packages.urllib3.packages.ssl_match_hostname', 'requests.packages.urllib3.packages.six', 'requests.packages.urllib3.packages.six.moves', 'requests.packages.urllib3.packages.six.moves.http_client', 'requests.packages.urllib3.connection', 'requests.packages.urllib3.util', 'requests.packages.urllib3.util.connection', 'requests.packages.urllib3.util.wait', 'requests.packages.urllib3.util.selectors', 'requests.packages.urllib3.util.request', 'requests.packages.urllib3.util.response', 'requests.packages.urllib3.util.ssl_', 'requests.packages.urllib3.util.timeout', 'requests.packages.urllib3.util.retry', 'requests.packages.urllib3.util.url', 'requests.packages.urllib3._collections', 'requests.packages.urllib3.request', 'requests.packages.urllib3.filepost', 'requests.packages.urllib3.fields', 'requests.packages.urllib3.packages.six.moves.urllib', 'requests.packages.urllib3.packages.six.moves.urllib.parse', 'requests.packages.urllib3.response', 'requests.packages.urllib3.poolmanager', 'requests.packages.urllib3.contrib', 'idna', 'idna.package_data', 'idna.core', 'idna.idnadata', 'idna.intranges', 'requests.packages.idna', 'requests.packages.idna.package_data', 'requests.packages.idna.core', 'requests.packages.idna.idnadata', 'requests.packages.idna.intranges', 'requests.packages.chardet', 'requests.packages.chardet.compat', 'requests.packages.chardet.universaldetector', 'requests.packages.chardet.charsetgroupprober', 'requests.packages.chardet.enums', 'requests.packages.chardet.charsetprober', 'requests.packages.chardet.escprober', 'requests.packages.chardet.codingstatemachine', 'requests.packages.chardet.escsm', 'requests.packages.chardet.latin1prober', 'requests.packages.chardet.mbcsgroupprober', 'requests.packages.chardet.utf8prober', 'requests.packages.chardet.mbcssm', 'requests.packages.chardet.sjisprober', 'requests.packages.chardet.mbcharsetprober', 'requests.packages.chardet.chardistribution', 'requests.packages.chardet.euctwfreq', 'requests.packages.chardet.euckrfreq', 'requests.packages.chardet.gb2312freq', 'requests.packages.chardet.big5freq', 'requests.packages.chardet.jisfreq', 'requests.packages.chardet.jpcntx', 'requests.packages.chardet.eucjpprober', 'requests.packages.chardet.gb2312prober', 'requests.packages.chardet.euckrprober', 'requests.packages.chardet.cp949prober', 'requests.packages.chardet.big5prober', 'requests.packages.chardet.euctwprober', 'requests.packages.chardet.sbcsgroupprober', 'requests.packages.chardet.sbcharsetprober', 'requests.packages.chardet.langcyrillicmodel', 'requests.packages.chardet.langgreekmodel', 'requests.packages.chardet.langbulgarianmodel', 'requests.packages.chardet.langthaimodel', 'requests.packages.chardet.langhebrewmodel', 'requests.packages.chardet.hebrewprober', 'requests.packages.chardet.langturkishmodel', 'requests.packages.chardet.version', 'requests.models', 'encodings.idna', 'stringprep', 'requests.hooks', 'requests.auth', 'requests.status_codes', 'requests.api', 'requests.sessions', 'requests.adapters', 'tqdm', 'tqdm._tqdm', 'tqdm._utils', 'tqdm._monitor', 'multiprocessing.synchronize', 'tqdm._tqdm_gui', 'tqdm._tqdm_pandas', 'tqdm._main', 'tqdm._version', 'deeppavlov.core.common', 'deeppavlov.core.common.log', 'logging.config', 'socketserver', 'deeppavlov.core.commands', 'deeppavlov.core.commands.utils', 'deeppavlov.core.common.paths', 'netrc', 'deeppavlov.dataset_readers', 'deeppavlov.dataset_readers.basic_classification_reader', 'pandas', 'pytz', 'pytz.exceptions', 'pytz.lazy', 'pytz.tzinfo', 'pytz.tzfile', 'pandas.compat', 'distutils', 'distutils.version', 'pandas.compat.chainmap', 'pandas.compat.numpy', 'pandas._libs', 'pandas._libs.tslib', 'pandas._libs.tslibs', 'pandas._libs.tslibs.conversion', 'pandas._libs.tslibs.np_datetime', '_cython_0_28_3', 'pandas._libs.tslibs.nattype', 'pandas._libs.tslibs.timedeltas', 'pandas._libs.tslibs.timezones', 'pandas._libs.tslibs.parsing', 'pandas._libs.tslibs.ccalendar', 'pandas._libs.tslibs.strptime', 'pandas._libs.tslibs.timestamps', 'pandas._libs.tslibs.fields', 'pandas._libs.hashtable', 'pandas._libs.missing', 'pandas._libs.lib', 'pandas.core', 'pandas.core.config_init', 'pandas.core.config', 'pandas.io', 'pandas.io.formats', 'pandas.io.formats.printing', 'pandas.core.dtypes', 'pandas.core.dtypes.inference', 'pandas.io.formats.console', 'pandas.io.formats.terminal', 'pandas.core.api', 'pandas.core.algorithms', 'pandas.core.dtypes.cast', 'pandas.core.dtypes.common', 'pandas._libs.algos', 'pandas.core.dtypes.dtypes', 'pandas.core.dtypes.generic', 'pandas.core.dtypes.base', 'pandas.errors', 'pandas.core.dtypes.missing', 'pandas.core.common', 'pandas.util', 'pandas.util._decorators', 'pandas._libs.properties', 'pandas.core.util', 'pandas.core.util.hashing', 'pandas._libs.hashing', 'pandas.core.arrays', 'pandas.core.arrays.base', 'pandas.compat.numpy.function', 'pandas.util._validators', 'pandas.core.arrays.categorical', 'pandas.core.accessor', 'pandas.core.base', 'pandas.core.nanops', 'pandas.core.missing', 'pandas.core.groupby', 'pandas.core.groupby.groupby', 'pandas.core.index', 'pandas.core.indexes', 'pandas.core.indexes.api', 'pandas.core.indexes.base', 'pandas._libs.index', 'pandas._libs.tslibs.period', 'pandas._libs.tslibs.frequencies', 'pandas._libs.tslibs.resolution', 'pandas.tseries', 'pandas.tseries.offsets', 'pandas.core.tools', 'pandas.core.tools.datetimes', 'dateutil.easter', 'pandas._libs.tslibs.offsets', 'pandas.tseries.frequencies', 'pandas._libs.join', 'pandas.core.ops', 'pandas._libs.ops', 'pandas.core.indexes.frozen', 'pandas.core.dtypes.concat', 'pandas.core.sorting', 'pandas.core.strings', 'pandas.core.indexes.category', 'pandas.core.indexes.multi', 'pandas.core.indexes.interval', 'pandas._libs.interval', 'pandas.core.indexes.datetimes', 'pandas.core.indexes.numeric', 'pandas.core.indexes.datetimelike', 'pandas.core.tools.timedeltas', 'pandas.core.indexes.timedeltas', 'pandas.core.indexes.range', 'pandas.core.indexes.period', 'pandas.core.frame', 'pandas.core.generic', 'pandas.core.indexing', 'pandas._libs.indexing', 'pandas.core.internals', 'pandas._libs.internals', 'pandas.core.sparse', 'pandas.core.sparse.array', 'pandas._libs.sparse', 'pandas.io.formats.format', 'pandas.io.common', 'csv', '_csv', 'mmap', 'pandas.core.series', 'pandas.core.indexes.accessors', 'pandas.plotting', 'pandas.plotting._misc', 'pandas.plotting._style', 'pandas.plotting._compat', 'pandas.plotting._tools', 'pandas.plotting._core', 'pandas.plotting._converter', 'matplotlib', 'matplotlib.cbook', 'matplotlib.cbook.deprecation', 'matplotlib.rcsetup', 'matplotlib.fontconfig_pattern', 'pyparsing', 'matplotlib.colors', 'matplotlib._color_data', 'cycler', 'matplotlib._version']\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.dataset_readers.basic_classification_reader import BasicClassificationDatasetReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:13:56.442 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find snips/valid.csv file\n",
      "2018-10-29 15:13:56.443 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find snips/test.csv file\n"
     ]
    }
   ],
   "source": [
    "# read data from particular columns of `.csv` file\n",
    "dr = BasicClassificationDatasetReader().read(\n",
    "    data_path='./snips/',\n",
    "    train='train.csv',\n",
    "    x = 'text',\n",
    "    y = 'intents'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have a ready train/valid/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('train', 15884), ('valid', 0), ('test', 0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check train/valid/test sizes\n",
    "[(k, len(dr[k])) for k in dr.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatasetIterator\n",
    "\n",
    "Use `BasicClassificationDatasetIterator` to split `train` on `train` and `valid` and to generate batches of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.dataset_iterators.basic_classification_iterator import BasicClassificationDatasetIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:14:47.555 INFO in 'deeppavlov.dataset_iterators.basic_classification_iterator'['basic_classification_iterator'] at line 73: Splitting field <<train>> to new fields <<['train', 'valid']>>\n"
     ]
    }
   ],
   "source": [
    "# initialize data iterator splitting `train` field to `train` and `valid` in proportion 9/1\n",
    "train_iterator = BasicClassificationDatasetIterator(data=dr,\n",
    "                                                    field_to_split='train',\n",
    "                                                    split_fields=['train', 'valid'],\n",
    "                                                    split_proportions=[0.8, 0.2],\n",
    "                                                    split_seed=23,\n",
    "                                                    seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look into training samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: Is it freezing in Offerman, California?\n",
      "y: ['GetWeather']\n",
      "=================\n",
      "x: put this song in the playlist Trap Land\n",
      "y: ['AddToPlaylist']\n",
      "=================\n",
      "x: show me a textbook with a rating of 2 and a maximum rating of 6 that is current\n",
      "y: ['RateBook']\n",
      "=================\n",
      "x: Will the weather be okay in Northern Luzon Heroes Hill National Park 4 and a half months from now?\n",
      "y: ['GetWeather']\n",
      "=================\n",
      "x: Rate the current album a four\n",
      "y: ['RateBook']\n",
      "=================\n"
     ]
    }
   ],
   "source": [
    "# one can get train instances (or any other data type including `all`)\n",
    "x_train, y_train = train_iterator.get_instances(data_type='train')\n",
    "for x, y in list(zip(x_train, y_train))[:5]:\n",
    "    print('x:', x)\n",
    "    print('y:', y)\n",
    "    print('=================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using lowercasing and tokenization as data preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`StrLower` lowercases texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.preprocessors.str_lower import StrLower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is it freezing in offerman, california?']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_lower = StrLower()\n",
    "str_lower(['Is it freezing in Offerman, California?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "`NLTKTokenizer` can split string to tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.models.tokenizers.nltk_moses_tokenizer import NLTKMosesTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Is', 'it', 'freezing', 'in', 'Offerman', ',', 'California', '?']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = NLTKMosesTokenizer()\n",
    "tokenizer(['Is it freezing in Offerman, California?'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's preprocess all `train` part of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_lower_tokenized = str_lower(tokenizer(train_iterator.get_instances(data_type='train')[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "\n",
    "Now we are ready to use `vocab`. They are very usefull for:\n",
    "* extracting class labels and converting labels to indices and vice versa,\n",
    "* building of characters or tokens vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.data.simple_vocab import SimpleVocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:16:12.681 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n"
     ]
    }
   ],
   "source": [
    "# initialize simple vocabulary to collect all occured in the dataset classes\n",
    "classes_vocab = SimpleVocabulary(\n",
    "    save_path='./snips/classes.dict',\n",
    "    load_path='./snips/classes.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:16:14.938 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 86: [saving vocabulary to snips/classes.dict]\n"
     ]
    }
   ],
   "source": [
    "classes_vocab.fit((train_iterator.get_instances(data_type='train')[1]))\n",
    "classes_vocab.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what classes the dataset contains and how many examples for each class we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GetWeather', 0),\n",
       " ('PlayMusic', 1),\n",
       " ('SearchScreeningEvent', 2),\n",
       " ('BookRestaurant', 3),\n",
       " ('RateBook', 4),\n",
       " ('SearchCreativeWork', 5),\n",
       " ('AddToPlaylist', 6)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(classes_vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:17:24.959 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/tokens.dict]\n"
     ]
    }
   ],
   "source": [
    "# also one can collect vocabulary of textual tokens appeared 2 and more times in the dataset\n",
    "token_vocab = SimpleVocabulary(\n",
    "    save_path='./snips/tokens.dict',\n",
    "    load_path='./snips/tokens.dict',\n",
    "    min_freq=2,\n",
    "    special_tokens=('<PAD>', '<UNK>',),\n",
    "    unk_token='<UNK>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:17:28.606 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 86: [saving vocabulary to snips/tokens.dict]\n"
     ]
    }
   ],
   "source": [
    "token_vocab.fit(train_x_lower_tokenized)\n",
    "token_vocab.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4564"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tokens in dictionary\n",
    "len(token_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 6953),\n",
       " ('a', 3917),\n",
       " ('in', 3265),\n",
       " ('to', 3203),\n",
       " ('for', 2814),\n",
       " ('of', 2401),\n",
       " ('.', 2400),\n",
       " ('i', 2079),\n",
       " ('at', 1935),\n",
       " ('play', 1703)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 most common words and number of times their appeared\n",
    "token_vocab.freqs.most_common()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[13, 36, 244, 4, 1, 29, 996, 20]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = token_vocab(str_lower(tokenizer(['Is it freezing in Offerman, California?'])))\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['is it freezing in <UNK>, california?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(token_vocab(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featurization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words\n",
    "\n",
    "Matches a vector to each text sample: text -> binary vector $v$: \\[0, 1, 0, 0, 0, 1, ..., ...1, 0, 1\\]. \n",
    "\n",
    "Dimensionality of vector $v$ is equal to vocabulary size.\n",
    "\n",
    "$v_i$ == 1, if word $i$ is in the text,\n",
    "\n",
    "$v_i$ == 0, else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from deeppavlov.models.embedders.bow_embedder import BoWEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0, 1, 0, ..., 0, 0, 0], dtype=int32)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize bag-of-words embedder giving total number of tokens\n",
    "bow = BoWEmbedder(depth=token_vocab.len)\n",
    "# it assumes indexed tokenized samples\n",
    "bow(token_vocab(str_lower(tokenizer(['Is it freezing in Offerman, California?']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all 8 tokens are in the vocabulary\n",
    "sum(bow(token_vocab(str_lower(tokenizer(['Is it freezing in Offerman, California?']))))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizer\n",
    "\n",
    "Matches a vector to each text sample: text -> vector $v$ from $R^N$ where $N$ is a vocabulary size.\n",
    "\n",
    "$TF-IDF(token, document) = TF(token, document) * IDF(token, document)$\n",
    "\n",
    "$TF$ is a term frequency:\n",
    "\n",
    "$TF(token, document) = \\frac{n_{token}}{\\sum_{k}n_k}.$\n",
    "\n",
    "$IDF$ is a inverse document frequency:\n",
    "\n",
    "$IDF(token, all\\_documents) = \\frac{всего\\ документов}{число\\ документов\\ в\\ которых\\ встретился\\ token}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SklearnComponent` in DeepPavlov is a universal wrapper for any vecotirzer/estimator from `sklearn` package. The only requirement to specify component usage is following: model class and name of infer method should be passed as parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.models.sklearn import SklearnComponent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:21:35.119 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 217: Cannot load model from tfidf_v0.pkl\n",
      "2018-10-29 15:21:35.121 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 164: Initializing model sklearn.feature_extraction.text:TfidfVectorizer from scratch\n"
     ]
    }
   ],
   "source": [
    "# initialize TF-IDF vectorizer sklearn component with `transform` as infer method\n",
    "tfidf = SklearnComponent(\n",
    "    model_class=\"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "    infer_method=\"transform\",\n",
    "    save_path='./tfidf_v0.pkl',\n",
    "    load_path='./tfidf_v0.pkl',\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:22:00.409 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 107: Fitting model sklearn.feature_extraction.text:TfidfVectorizer\n",
      "2018-10-29 15:22:00.512 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 239: Saving model to tfidf_v0.pkl\n"
     ]
    }
   ],
   "source": [
    "# fit on textual train instances and save it\n",
    "tfidf.fit(str_lower(train_iterator.get_instances(data_type='train')[0]))\n",
    "tfidf.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x10709 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf(str_lower(['Is it freezing in Offerman, California?']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10709"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.model.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastText embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.models.embedders.fasttext_embedder import FasttextEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_download(url=\"http://files.deeppavlov.ai/deeppavlov_data/embeddings/wiki.en.bin\", \n",
    "                destination=\"./wiki.en.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:28:11.569 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 52: [loading fastText embeddings from `wiki.en.bin`]\n"
     ]
    }
   ],
   "source": [
    "embedder = FasttextEmbedder(load_path='./wiki.en.bin',\n",
    "                            dim=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, (300,))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output shape is (batch_size x num_tokens x embedding_dim)\n",
    "embedded_batch = embedder(str_lower(tokenizer(['Is it freezing in Offerman, California?']))) \n",
    "len(embedded_batch), len(embedded_batch[0]), embedded_batch[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fastText weighted by TF-IDF embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.models.embedders.tfidf_weighted_embedder import TfidfWeightedEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_embedder = TfidfWeightedEmbedder(\n",
    "    embedder=embedder,  # our fastText embedder instance\n",
    "    tokenizer=tokenizer,  # out tokenizer instance\n",
    "    mean=False,  # whether to return one vector per sample or embed every token separately\n",
    "    vectorizer=tfidf  # our TF-IDF vectorizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 8, (300,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output shape is (batch_size x num_tokens x embedding_dim)\n",
    "embedded_batch = weighted_embedder(str_lower(tokenizer(['Is it freezing in Offerman, California?']))) \n",
    "len(embedded_batch), len(embedded_batch[0]), embedded_batch[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean fastText embedder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedder returns a vector per token while we want to get a vector per text sample. Therefore, let's calculate mean vector of embeddings of tokens. \n",
    "For that we can either init `FasttextEmbedder` with `mean=True` parameter (`mean=false` by default), or pass `mean=true` while calling function (this way `mean` value is assigned only for this call)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, (300,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output shape is (batch_size x embedding_dim)\n",
    "embedded_batch = embedder(str_lower(tokenizer(['Is it freezing in Offerman, California?'])), mean=True) \n",
    "len(embedded_batch), embedded_batch[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.metrics.accuracy import sets_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all train and valid data from iterator\n",
    "x_train, y_train = train_iterator.get_instances(data_type=\"train\")\n",
    "x_valid, y_valid = train_iterator.get_instances(data_type=\"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SklearnComponent classifier on Tfidf-features in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:31:41.379 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 217: Cannot load model from logreg_v0.pkl\n",
      "2018-10-29 15:31:41.382 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 164: Initializing model sklearn.linear_model:LogisticRegression from scratch\n"
     ]
    }
   ],
   "source": [
    "# initialize sklearn classifier, all parameters for classifier could be passed\n",
    "cls = SklearnComponent(\n",
    "    model_class=\"sklearn.linear_model:LogisticRegression\",\n",
    "    infer_method=\"predict\",\n",
    "    save_path='./logreg_v0.pkl',\n",
    "    load_path='./logreg_v0.pkl',\n",
    "    C=1,\n",
    "    mode='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:31:42.706 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 107: Fitting model sklearn.linear_model:LogisticRegression\n",
      "2018-10-29 15:31:42.897 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 239: Saving model to logreg_v0.pkl\n"
     ]
    }
   ],
   "source": [
    "# fit sklearn classifier and save it\n",
    "cls.fit(tfidf(x_train), y_train)\n",
    "cls.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid_pred = cls(tfidf(x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.982373308152345"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's calculate sets accuracy (because each element is a list of labels)\n",
    "sets_accuracy(np.squeeze(y_valid), y_valid_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KerasClassificationModel on fastText embeddings in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from deeppavlov.models.classifiers.keras_classification_model import KerasClassificationModel\n",
    "from deeppavlov.models.preprocessors.one_hotter import OneHotter\n",
    "from deeppavlov.models.classifiers.proba2labels import Proba2Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:32:03.788 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 242: [initializing `KerasClassificationModel` from scratch as cnn_model]\n",
      "2018-10-29 15:32:04.160 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 128)      115328      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 128)      192128      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 128)      268928      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 128)      512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 128)      512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 128)      512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 128)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 128)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 128)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 384)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          38500       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7)            28          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 617,555\n",
      "Trainable params: 616,573\n",
      "Non-trainable params: 982\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Intialize `KerasClassificationModel` that composes CNN shallow-and-wide network \n",
    "# (name here as`cnn_model`)\n",
    "cls = KerasClassificationModel(save_path=\"./cnn_model_v0\", \n",
    "                               load_path=\"./cnn_model_v0\", \n",
    "                               embedding_size=embedder.dim,\n",
    "                               n_classes=classes_vocab.len,\n",
    "                               model_name=\"cnn_model\",\n",
    "                               text_size=15, # number of tokens\n",
    "                               kernel_sizes_cnn=[3, 5, 7],\n",
    "                               filters_cnn=128,\n",
    "                               dense_size=100,\n",
    "                               optimizer=\"Adam\",\n",
    "                               learning_rate=0.1,\n",
    "                               learning_rate_decay=0.01,\n",
    "                               loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `KerasClassificationModel` assumes one-hotted distribution of classes per sample.\n",
    "# `OneHotter` converts indices to one-hot vectors representation.\n",
    "#  To obtain indices we can use our `classes_vocab` intialized and fitted above\n",
    "onehotter = OneHotter(depth=classes_vocab.len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 10 epochs\n",
    "for ep in range(10):\n",
    "    for x, y in train_iterator.gen_batches(batch_size=64, \n",
    "                                           data_type=\"train\"):\n",
    "        x_embed = embedder(tokenizer(str_lower(x)))\n",
    "        y_onehot = onehotter(classes_vocab(y))\n",
    "        cls.train_on_batch(x_embed, y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:32:28.509 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v0_opt.json]\n"
     ]
    }
   ],
   "source": [
    "# Save model weights and parameters\n",
    "cls.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0001224217121489346,\n",
       " 5.4968368203844875e-05,\n",
       " 0.00048470927868038416,\n",
       " 0.9979118704795837,\n",
       " 0.00019703485304489732,\n",
       " 0.0011488927993923426,\n",
       " 0.0005156921688467264]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Infering on validation data we get probability distribution on given data.\n",
    "y_valid_pred = cls(embedder(tokenizer(str_lower(x_valid))))\n",
    "y_valid_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert probability distribution to labels, \n",
    "# we first need to convert probabilities to indices,\n",
    "# and then using vocabulary `classes_vocab` convert indices to labels.\n",
    "# \n",
    "# `Proba2Labels` converts probabilities to indices and supports three different modes:\n",
    "# if `max_proba` is true, returns indices of the highest probabilities\n",
    "# if `confident_threshold` is given, returns indices with probabiltiies higher than threshold\n",
    "# if `top_n` is given, returns `top_n` indices with highest probabilities\n",
    "prob2labels = Proba2Labels(max_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9889833175952156"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate sets accuracy\n",
    "sets_accuracy(y_valid, classes_vocab(prob2labels(y_valid_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KerasClassificationModel on fastText weighted by TF-IDF embeddings in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:43:10.787 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 242: [initializing `KerasClassificationModel` from scratch as cnn_model]\n",
      "2018-10-29 15:43:11.134 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 128)      115328      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 128)      192128      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 128)      268928      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 128)      512         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 128)      512         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 128)      512         conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 128)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 128)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 128)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 128)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 128)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 128)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 384)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          38500       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7)            28          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 617,555\n",
      "Trainable params: 616,573\n",
      "Non-trainable params: 982\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Intialize `KerasClassificationModel` that composes CNN shallow-and-wide network \n",
    "# (name here as`cnn_model`)\n",
    "cls = KerasClassificationModel(save_path=\"./cnn_model_v1\", \n",
    "                               load_path=\"./cnn_model_v1\", \n",
    "                               embedding_size=weighted_embedder.dim,\n",
    "                               n_classes=classes_vocab.len,\n",
    "                               model_name=\"cnn_model\",\n",
    "                               text_size=15, # number of tokens\n",
    "                               kernel_sizes_cnn=[3, 5, 7],\n",
    "                               filters_cnn=128,\n",
    "                               dense_size=100,\n",
    "                               optimizer=\"Adam\",\n",
    "                               learning_rate=0.1,\n",
    "                               learning_rate_decay=0.01,\n",
    "                               loss=\"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `KerasClassificationModel` assumes one-hotted distribution of classes per sample.\n",
    "# `OneHotter` converts indices to one-hot vectors representation.\n",
    "#  To obtain indices we can use our `classes_vocab` intialized and fitted above\n",
    "onehotter = OneHotter(depth=classes_vocab.len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train for 10 epochs\n",
    "for ep in range(10):\n",
    "    for x, y in train_iterator.gen_batches(batch_size=64, \n",
    "                                           data_type=\"train\"):\n",
    "        x_embed = weighted_embedder(tokenizer(str_lower(x)))\n",
    "        y_onehot = onehotter(classes_vocab(y))\n",
    "        cls.train_on_batch(x_embed, y_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 15:47:03.867 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v1_opt.json]\n"
     ]
    }
   ],
   "source": [
    "cls.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8.803312084637582e-05,\n",
       " 0.00012861542927566916,\n",
       " 0.0001089772122213617,\n",
       " 0.9999517202377319,\n",
       " 0.00046508893137797713,\n",
       " 0.00012267997954040766,\n",
       " 0.00035249924985691905]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Infering on validation data we get probability distribution on given data.\n",
    "y_valid_pred = cls(weighted_embedder(tokenizer(str_lower(x_valid))))\n",
    "y_valid_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To convert probability distribution to labels, \n",
    "# we first need to convert probabilities to indices,\n",
    "# and then using vocabulary `classes_vocab` convert indices to labels.\n",
    "# \n",
    "# `Proba2Labels` converts probabilities to indices and supports three different modes:\n",
    "# if `max_proba` is true, returns indices of the highest probabilities\n",
    "# if `confident_threshold` is given, returns indices with probabiltiies higher than threshold\n",
    "# if `top_n` is given, returns `top_n` indices with highest probabilities\n",
    "prob2labels = Proba2Labels(max_proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9798552093169657"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate sets accuracy\n",
    "sets_accuracy(y_valid, classes_vocab(prob2labels(y_valid_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models from configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeppavlov.core.commands.infer import build_model_from_config\n",
    "from deeppavlov.core.commands.train import train_evaluate_model_from_config, _test_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SklearnComponent classifier on Tfidf-features from config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_config = {\"deeppavlov_root\": \".\",\n",
    "  \"dataset_reader\": {\n",
    "    \"name\": \"basic_classification_reader\",\n",
    "    \"x\": \"text\",\n",
    "    \"y\": \"intents\",\n",
    "    \"data_path\": \"./snips\"\n",
    "  },\n",
    "  \"dataset_iterator\": {\n",
    "    \"name\": \"basic_classification_iterator\",\n",
    "    \"seed\": 42,\n",
    "    \"split_seed\": 23,\n",
    "    \"field_to_split\": \"train\",\n",
    "    \"split_fields\": [\n",
    "      \"train\",\n",
    "      \"valid\"\n",
    "    ],\n",
    "    \"split_proportions\": [\n",
    "      0.9,\n",
    "      0.1\n",
    "    ]\n",
    "  },\n",
    "  \"chainer\": {\n",
    "    \"in\": [\n",
    "      \"x\"\n",
    "    ],\n",
    "    \"in_y\": [\n",
    "      \"y\"\n",
    "    ],\n",
    "    \"pipe\": [\n",
    "      {\n",
    "        \"id\": \"classes_vocab\",\n",
    "        \"name\": \"simple_vocab\",\n",
    "        \"fit_on\": [\n",
    "          \"y\"\n",
    "        ],\n",
    "        \"save_path\": \"./snips/classes.dict\",\n",
    "        \"load_path\": \"./snips/classes.dict\",\n",
    "        \"in\": \"y\",\n",
    "        \"out\": \"y_ids\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": [\n",
    "          \"x\"\n",
    "        ],\n",
    "        \"out\": [\n",
    "          \"x_vec\"\n",
    "        ],\n",
    "        \"fit_on\": [\n",
    "          \"x\",\n",
    "          \"y_ids\"\n",
    "        ],\n",
    "        \"id\": \"tfidf_vec\",\n",
    "        \"name\": \"sklearn_component\",\n",
    "        \"save_path\": \"tfidf_v1.pkl\",\n",
    "        \"load_path\": \"tfidf_v1.pkl\",\n",
    "        \"model_class\": \"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "        \"infer_method\": \"transform\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"x\",\n",
    "        \"out\": \"x_tok\",\n",
    "        \"id\": \"my_tokenizer\",\n",
    "        \"name\": \"nltk_moses_tokenizer\",\n",
    "        \"tokenizer\": \"wordpunct_tokenize\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": [\n",
    "          \"x_vec\"\n",
    "        ],\n",
    "        \"out\": [\n",
    "          \"y_pred\"\n",
    "        ],\n",
    "        \"fit_on\": [\n",
    "          \"x_vec\",\n",
    "          \"y\"\n",
    "        ],\n",
    "        \"name\": \"sklearn_component\",\n",
    "        \"main\": True,\n",
    "        \"save_path\": \"logreg_v1.pkl\",\n",
    "        \"load_path\": \"logreg_v1.pkl\",\n",
    "        \"model_class\": \"sklearn.linear_model:LogisticRegression\",\n",
    "        \"infer_method\": \"predict\",\n",
    "        \"ensure_list_output\": True\n",
    "      }\n",
    "    ],\n",
    "    \"out\": [\n",
    "      \"y_pred\"\n",
    "    ]\n",
    "  },\n",
    "  \"train\": {\n",
    "    \"batch_size\": 64,\n",
    "    \"metrics\": [\n",
    "      \"accuracy\"\n",
    "    ],\n",
    "    \"validate_best\": True,\n",
    "    \"test_best\": False\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:03:26.561 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find snips/valid.csv file\n",
      "2018-10-29 16:03:26.562 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find snips/test.csv file\n",
      "2018-10-29 16:03:26.562 INFO in 'deeppavlov.dataset_iterators.basic_classification_iterator'['basic_classification_iterator'] at line 73: Splitting field <<train>> to new fields <<['train', 'valid']>>\n",
      "2018-10-29 16:03:26.565 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n",
      "2018-10-29 16:03:26.572 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 86: [saving vocabulary to snips/classes.dict]\n",
      "2018-10-29 16:03:26.573 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 217: Cannot load model from tfidf_v1.pkl\n",
      "2018-10-29 16:03:26.573 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 164: Initializing model sklearn.feature_extraction.text:TfidfVectorizer from scratch\n",
      "2018-10-29 16:03:26.599 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 107: Fitting model sklearn.feature_extraction.text:TfidfVectorizer\n",
      "2018-10-29 16:03:26.719 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 239: Saving model to tfidf_v1.pkl\n",
      "2018-10-29 16:03:26.736 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 217: Cannot load model from logreg_v1.pkl\n",
      "2018-10-29 16:03:26.736 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 164: Initializing model sklearn.linear_model:LogisticRegression from scratch\n",
      "2018-10-29 16:03:27.360 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 107: Fitting model sklearn.linear_model:LogisticRegression\n",
      "2018-10-29 16:03:27.580 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 239: Saving model to logreg_v1.pkl\n",
      "2018-10-29 16:03:27.581 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n",
      "2018-10-29 16:03:27.582 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.feature_extraction.text:TfidfVectorizer from tfidf_v1.pkl\n",
      "2018-10-29 16:03:27.587 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2018-10-29 16:03:27.587 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2018-10-29 16:03:27.589 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.linear_model:LogisticRegression from logreg_v1.pkl\n",
      "2018-10-29 16:03:27.589 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2018-10-29 16:03:27.590 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2018-10-29 16:03:27.591 INFO in 'deeppavlov.core.commands.train'['train'] at line 221: Testing the best saved model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"accuracy\": 0.983}, \"time_spent\": \"0:00:01\"}}\n"
     ]
    }
   ],
   "source": [
    "# we can train and evaluate model from config\n",
    "m = train_evaluate_model_from_config(logreg_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:03:27.689 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n",
      "2018-10-29 16:03:27.689 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.feature_extraction.text:TfidfVectorizer from tfidf_v1.pkl\n",
      "2018-10-29 16:03:27.694 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2018-10-29 16:03:27.694 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2018-10-29 16:03:27.696 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.linear_model:LogisticRegression from logreg_v1.pkl\n",
      "2018-10-29 16:03:27.697 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2018-10-29 16:03:27.697 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n"
     ]
    }
   ],
   "source": [
    "# or we can just load pre-trained model (conicides with what we did above)\n",
    "m = build_model_from_config(logreg_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['GetWeather']]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m([\"Is it freezing in Offerman, California?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KerasClassificationModel on fastText embeddings from config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_config = {\"deeppavlov_root\": \".\",\n",
    "  \"dataset_reader\": {\n",
    "    \"name\": \"basic_classification_reader\",\n",
    "    \"x\": \"text\",\n",
    "    \"y\": \"intents\",\n",
    "    \"data_path\": \"snips\"\n",
    "  },\n",
    "  \"dataset_iterator\": {\n",
    "    \"name\": \"basic_classification_iterator\",\n",
    "    \"seed\": 42,\n",
    "    \"split_seed\": 23,\n",
    "    \"field_to_split\": \"train\",\n",
    "    \"split_fields\": [\n",
    "      \"train\",\n",
    "      \"valid\"\n",
    "    ],\n",
    "    \"split_proportions\": [\n",
    "      0.9,\n",
    "      0.1\n",
    "    ]\n",
    "  },\n",
    "  \"chainer\": {\n",
    "    \"in\": [\n",
    "      \"x\"\n",
    "    ],\n",
    "    \"in_y\": [\n",
    "      \"y\"\n",
    "    ],\n",
    "    \"pipe\": [\n",
    "      {\n",
    "        \"id\": \"classes_vocab\",\n",
    "        \"name\": \"simple_vocab\",\n",
    "        \"fit_on\": [\n",
    "          \"y\"\n",
    "        ],\n",
    "        \"level\": \"token\",\n",
    "        \"save_path\": \"./snips/classes.dict\",\n",
    "        \"load_path\": \"./snips/classes.dict\",\n",
    "        \"in\": \"y\",\n",
    "        \"out\": \"y_ids\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"x\",\n",
    "        \"out\": \"x_tok\",\n",
    "        \"id\": \"my_tokenizer\",\n",
    "        \"name\": \"nltk_tokenizer\",\n",
    "        \"tokenizer\": \"wordpunct_tokenize\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"x_tok\",\n",
    "        \"out\": \"x_emb\",\n",
    "        \"id\": \"my_embedder\",\n",
    "        \"name\": \"fasttext\",\n",
    "        \"load_path\": \"./wiki.en.bin\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"y_ids\",\n",
    "        \"out\": \"y_onehot\",\n",
    "        \"name\": \"one_hotter\",\n",
    "        \"depth\": \"#classes_vocab.len\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": [\n",
    "          \"x_emb\"\n",
    "        ],\n",
    "        \"in_y\": [\n",
    "          \"y_onehot\"\n",
    "        ],\n",
    "        \"out\": [\n",
    "          \"y_pred_probas\"\n",
    "        ],\n",
    "        \"main\": True,\n",
    "        \"name\": \"keras_classification_model\",\n",
    "        \"save_path\": \"./cnn_model_v2\",\n",
    "        \"load_path\": \"./cnn_model_v2\",\n",
    "        \"embedding_size\": \"#my_embedder.dim\",\n",
    "        \"n_classes\": \"#classes_vocab.len\",\n",
    "        \"kernel_sizes_cnn\": [\n",
    "          1,\n",
    "          2,\n",
    "          3\n",
    "        ],\n",
    "        \"filters_cnn\": 256,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"learning_rate_decay\": 0.1,\n",
    "        \"loss\": \"categorical_crossentropy\",\n",
    "        \"text_size\": 15,\n",
    "        \"coef_reg_cnn\": 1e-4,\n",
    "        \"coef_reg_den\": 1e-4,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"dense_size\": 100,\n",
    "        \"model_name\": \"cnn_model\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"y_pred_probas\",\n",
    "        \"out\": \"y_pred_ids\",\n",
    "        \"name\": \"proba2labels\",\n",
    "        \"max_proba\": True\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"y_pred_ids\",\n",
    "        \"out\": \"y_pred_labels\",\n",
    "        \"ref\": \"classes_vocab\"\n",
    "      }\n",
    "    ],\n",
    "    \"out\": [\n",
    "      \"y_pred_labels\"\n",
    "    ]\n",
    "  },\n",
    "  \"train\": {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"metrics\": [\n",
    "      \"sets_accuracy\",\n",
    "      \"f1_macro\",\n",
    "      {\n",
    "        \"name\": \"roc_auc\",\n",
    "        \"inputs\": [\"y_onehot\", \"y_pred_probas\"]\n",
    "      }\n",
    "    ],\n",
    "    \"validation_patience\": 5,\n",
    "    \"val_every_n_epochs\": 1,\n",
    "    \"log_every_n_epochs\": 1,\n",
    "    \"show_examples\": True,\n",
    "    \"validate_best\": True,\n",
    "    \"test_best\": False\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:09:09.863 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find snips/valid.csv file\n",
      "2018-10-29 16:09:09.864 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find snips/test.csv file\n",
      "2018-10-29 16:09:09.865 INFO in 'deeppavlov.dataset_iterators.basic_classification_iterator'['basic_classification_iterator'] at line 73: Splitting field <<train>> to new fields <<['train', 'valid']>>\n",
      "2018-10-29 16:09:09.871 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n",
      "2018-10-29 16:09:09.879 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 86: [saving vocabulary to snips/classes.dict]\n",
      "2018-10-29 16:09:09.880 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 52: [loading fastText embeddings from `wiki.en.bin`]\n",
      "2018-10-29 16:09:52.603 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 242: [initializing `KerasClassificationModel` from scratch as cnn_model]\n",
      "2018-10-29 16:09:53.31 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 256)      77056       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 256)      153856      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 256)      230656      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 256)      1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 256)      1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 256)      1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 256)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 256)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 256)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7)            28          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 542,675\n",
      "Trainable params: 540,925\n",
      "Non-trainable params: 1,750\n",
      "__________________________________________________________________________________________________\n",
      "/home/dilyara/anaconda3/envs/deep36_reserve/lib/python3.6/site-packages/scikit_learn-0.19.1-py3.6-linux-x86_64.egg/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "2018-10-29 16:09:53.375 INFO in 'deeppavlov.core.commands.train'['train'] at line 360: New best sets_accuracy of 0.134\n",
      "2018-10-29 16:09:53.375 INFO in 'deeppavlov.core.commands.train'['train'] at line 362: Saving model\n",
      "2018-10-29 16:09:53.375 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v2_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.134, \"f1_macro\": 0.0501, \"roc_auc\": 0.5041}, \"time_spent\": \"0:00:01\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 0, \"batches_seen\": 0, \"train_examples_seen\": 0, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:09:57.924 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9465\n",
      "2018-10-29 16:09:57.925 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-10-29 16:09:57.925 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v2_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 1, \"batches_seen\": 224, \"train_examples_seen\": 14295, \"metrics\": {\"sets_accuracy\": 0.9, \"f1_macro\": 0.8994, \"roc_auc\": 0.9846}, \"time_spent\": \"0:00:05\", \"examples\": [{\"x\": \"Add lisa m to my guitar hero live playlist\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"AddToPlaylist\"]}], \"loss\": 1.4812453889421053}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9465, \"f1_macro\": 0.9459, \"roc_auc\": 0.9959}, \"time_spent\": \"0:00:05\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 1, \"batches_seen\": 224, \"train_examples_seen\": 14295, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:10:00.528 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9578\n",
      "2018-10-29 16:10:00.528 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-10-29 16:10:00.529 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v2_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 2, \"batches_seen\": 448, \"train_examples_seen\": 28590, \"metrics\": {\"sets_accuracy\": 0.9582, \"f1_macro\": 0.9583, \"roc_auc\": 0.9974}, \"time_spent\": \"0:00:08\", \"examples\": [{\"x\": \"I give The Monkey and the Tiger a rating of 2 points.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"RateBook\"]}], \"loss\": 1.337195518293551}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9578, \"f1_macro\": 0.9572, \"roc_auc\": 0.9972}, \"time_spent\": \"0:00:08\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 2, \"batches_seen\": 448, \"train_examples_seen\": 28590, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:10:03.231 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9635\n",
      "2018-10-29 16:10:03.232 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-10-29 16:10:03.232 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v2_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 3, \"batches_seen\": 672, \"train_examples_seen\": 42885, \"metrics\": {\"sets_accuracy\": 0.9668, \"f1_macro\": 0.9669, \"roc_auc\": 0.9982}, \"time_spent\": \"0:00:11\", \"examples\": [{\"x\": \"play Iheart tunes by Neil Finn\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"PlayMusic\"]}], \"loss\": 1.2842914696250642}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9635, \"f1_macro\": 0.9626, \"roc_auc\": 0.9977}, \"time_spent\": \"0:00:11\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 3, \"batches_seen\": 672, \"train_examples_seen\": 42885, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:10:05.811 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9679\n",
      "2018-10-29 16:10:05.811 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-10-29 16:10:05.812 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v2_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 4, \"batches_seen\": 896, \"train_examples_seen\": 57180, \"metrics\": {\"sets_accuracy\": 0.9717, \"f1_macro\": 0.9717, \"roc_auc\": 0.9986}, \"time_spent\": \"0:00:13\", \"examples\": [{\"x\": \"Please play a song off the Curtis Lee album Rough Diamonds\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"PlayMusic\"]}], \"loss\": 1.2475687728396483}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9679, \"f1_macro\": 0.9671, \"roc_auc\": 0.9979}, \"time_spent\": \"0:00:13\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 4, \"batches_seen\": 896, \"train_examples_seen\": 57180, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:10:08.438 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9692\n",
      "2018-10-29 16:10:08.439 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-10-29 16:10:08.439 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v2_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 5, \"batches_seen\": 1120, \"train_examples_seen\": 71475, \"metrics\": {\"sets_accuracy\": 0.975, \"f1_macro\": 0.9751, \"roc_auc\": 0.9988}, \"time_spent\": \"0:00:16\", \"examples\": [{\"x\": \"Give me Slovakia's weather forecast for eight am\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"GetWeather\"]}], \"loss\": 1.2212532593735628}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9692, \"f1_macro\": 0.9683, \"roc_auc\": 0.9981}, \"time_spent\": \"0:00:16\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 5, \"batches_seen\": 1120, \"train_examples_seen\": 71475, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:10:11.19 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9704\n",
      "2018-10-29 16:10:11.20 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-10-29 16:10:11.20 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v2_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 6, \"batches_seen\": 1344, \"train_examples_seen\": 85770, \"metrics\": {\"sets_accuracy\": 0.9758, \"f1_macro\": 0.9758, \"roc_auc\": 0.9989}, \"time_spent\": \"0:00:18\", \"examples\": [{\"x\": \"rate this current textbook 0 points\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"RateBook\"]}], \"loss\": 1.2003759257495403}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9704, \"f1_macro\": 0.9695, \"roc_auc\": 0.9982}, \"time_spent\": \"0:00:18\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 6, \"batches_seen\": 1344, \"train_examples_seen\": 85770, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:10:13.603 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9711\n",
      "2018-10-29 16:10:13.603 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-10-29 16:10:13.603 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v2_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 7, \"batches_seen\": 1568, \"train_examples_seen\": 100065, \"metrics\": {\"sets_accuracy\": 0.9771, \"f1_macro\": 0.9771, \"roc_auc\": 0.9991}, \"time_spent\": \"0:00:21\", \"examples\": [{\"x\": \"I need a bar for four that serves argentinian in D'Iberville, WY for twelve PM\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"loss\": 1.1837445806179727}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9711, \"f1_macro\": 0.9702, \"roc_auc\": 0.9984}, \"time_spent\": \"0:00:21\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 7, \"batches_seen\": 1568, \"train_examples_seen\": 100065, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:10:16.180 INFO in 'deeppavlov.core.commands.train'['train'] at line 518: New best sets_accuracy of 0.9729\n",
      "2018-10-29 16:10:16.180 INFO in 'deeppavlov.core.commands.train'['train'] at line 520: Saving model\n",
      "2018-10-29 16:10:16.181 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v2_opt.json]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 8, \"batches_seen\": 1792, \"train_examples_seen\": 114360, \"metrics\": {\"sets_accuracy\": 0.9778, \"f1_macro\": 0.9778, \"roc_auc\": 0.9991}, \"time_spent\": \"0:00:24\", \"examples\": [{\"x\": \"play The Sea Cabinet\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"SearchCreativeWork\"]}], \"loss\": 1.1712328317974294}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9729, \"f1_macro\": 0.9721, \"roc_auc\": 0.9984}, \"time_spent\": \"0:00:24\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 8, \"batches_seen\": 1792, \"train_examples_seen\": 114360, \"impatience\": 0, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:10:18.838 INFO in 'deeppavlov.core.commands.train'['train'] at line 525: Did not improve on the sets_accuracy of 0.9729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 9, \"batches_seen\": 2016, \"train_examples_seen\": 128655, \"metrics\": {\"sets_accuracy\": 0.9789, \"f1_macro\": 0.9789, \"roc_auc\": 0.9992}, \"time_spent\": \"0:00:26\", \"examples\": [{\"x\": \"add Ik Tara to laundry playlst\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"AddToPlaylist\"]}], \"loss\": 1.158443774495806}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9717, \"f1_macro\": 0.9709, \"roc_auc\": 0.9985}, \"time_spent\": \"0:00:26\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 9, \"batches_seen\": 2016, \"train_examples_seen\": 128655, \"impatience\": 1, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:10:21.388 INFO in 'deeppavlov.core.commands.train'['train'] at line 525: Did not improve on the sets_accuracy of 0.9729\n",
      "2018-10-29 16:10:21.726 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"train\": {\"epochs_done\": 10, \"batches_seen\": 2240, \"train_examples_seen\": 142950, \"metrics\": {\"sets_accuracy\": 0.9794, \"f1_macro\": 0.9795, \"roc_auc\": 0.9993}, \"time_spent\": \"0:00:29\", \"examples\": [{\"x\": \"Is it going to be hot in Karthaus at 7 AM?\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"GetWeather\"]}], \"loss\": 1.1475949878139156}}\n",
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9729, \"f1_macro\": 0.9721, \"roc_auc\": 0.9986}, \"time_spent\": \"0:00:29\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}], \"epochs_done\": 10, \"batches_seen\": 2240, \"train_examples_seen\": 142950, \"impatience\": 2, \"patience_limit\": 5}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:10:21.740 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 52: [loading fastText embeddings from `wiki.en.bin`]\n",
      "2018-10-29 16:11:22.285 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 272: [initializing `KerasClassificationModel` from saved]\n",
      "2018-10-29 16:11:22.621 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 282: [loading weights from cnn_model_v2.h5]\n",
      "2018-10-29 16:11:22.848 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 256)      77056       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 256)      153856      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 256)      230656      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 256)      1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 256)      1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 256)      1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 256)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 256)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 256)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7)            28          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 542,675\n",
      "Trainable params: 540,925\n",
      "Non-trainable params: 1,750\n",
      "__________________________________________________________________________________________________\n",
      "2018-10-29 16:11:22.852 INFO in 'deeppavlov.core.commands.train'['train'] at line 221: Testing the best saved model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9729, \"f1_macro\": 0.9721, \"roc_auc\": 0.9984}, \"time_spent\": \"0:00:01\", \"examples\": [{\"x\": \"Book a table at Carter House Inn in Saint Bonaventure, Alaska.\", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"BookRestaurant\"]}]}}\n"
     ]
    }
   ],
   "source": [
    "# we can train and evaluate model from config\n",
    "m = train_evaluate_model_from_config(cnn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:11:23.410 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n",
      "2018-10-29 16:11:23.424 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.feature_extraction.text:TfidfVectorizer from tfidf_v1.pkl\n",
      "2018-10-29 16:11:23.440 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2018-10-29 16:11:23.440 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2018-10-29 16:11:23.444 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.linear_model:LogisticRegression from logreg_v1.pkl\n",
      "2018-10-29 16:11:23.455 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.linear_model.logisticLogisticRegression loaded  with parameters\n",
      "2018-10-29 16:11:23.456 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n"
     ]
    }
   ],
   "source": [
    "# or we can just load pre-trained model (conicides with what we did above)\n",
    "m = build_model_from_config(logreg_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['GetWeather']]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m([\"Is it freezing in Offerman, California?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KerasClassificationModel on fastText weighted by TF-IDF embeddings from config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_config = {\"deeppavlov_root\": \".\",\n",
    "  \"dataset_reader\": {\n",
    "    \"name\": \"basic_classification_reader\",\n",
    "    \"x\": \"text\",\n",
    "    \"y\": \"intents\",\n",
    "    \"data_path\": \"snips\"\n",
    "  },\n",
    "  \"dataset_iterator\": {\n",
    "    \"name\": \"basic_classification_iterator\",\n",
    "    \"seed\": 42,\n",
    "      \"split_seed\": 23,\n",
    "    \"field_to_split\": \"train\",\n",
    "    \"split_fields\": [\n",
    "      \"train\",\n",
    "      \"valid\"\n",
    "    ],\n",
    "    \"split_proportions\": [\n",
    "      0.9,\n",
    "      0.1\n",
    "    ]\n",
    "  },\n",
    "  \"chainer\": {\n",
    "    \"in\": [\n",
    "      \"x\"\n",
    "    ],\n",
    "    \"in_y\": [\n",
    "      \"y\"\n",
    "    ],\n",
    "    \"pipe\": [\n",
    "      {\n",
    "        \"id\": \"classes_vocab\",\n",
    "        \"name\": \"simple_vocab\",\n",
    "        \"fit_on\": [\n",
    "          \"y\"\n",
    "        ],\n",
    "        \"save_path\": \"./snips/classes.dict\",\n",
    "        \"load_path\": \"./snips/classes.dict\",\n",
    "        \"in\": \"y\",\n",
    "        \"out\": \"y_ids\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": [\n",
    "          \"x\"\n",
    "        ],\n",
    "        \"out\": [\n",
    "          \"x_vec\"\n",
    "        ],\n",
    "        \"fit_on\": [\n",
    "          \"x\",\n",
    "          \"y_ids\"\n",
    "        ],\n",
    "        \"id\": \"my_tfidf_vectorizer\",\n",
    "        \"name\": \"sklearn_component\",\n",
    "        \"save_path\": \"tfidf_v2.pkl\",\n",
    "        \"load_path\": \"tfidf_v2.pkl\",\n",
    "        \"model_class\": \"sklearn.feature_extraction.text:TfidfVectorizer\",\n",
    "        \"infer_method\": \"transform\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"x\",\n",
    "        \"out\": \"x_tok\",\n",
    "        \"id\": \"my_tokenizer\",\n",
    "        \"name\": \"nltk_moses_tokenizer\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"x_tok\",\n",
    "        \"out\": \"x_emb\",\n",
    "        \"id\": \"my_embedder\",\n",
    "        \"name\": \"fasttext\",\n",
    "        \"save_path\": \"wiki.en.bin\",\n",
    "        \"load_path\": \"wiki.en.bin\",\n",
    "        \"dim\": 300\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"one_hotter\",\n",
    "        \"id\": \"my_onehotter\",\n",
    "        \"depth\": \"#classes_vocab.len\",\n",
    "        \"in\": \"y_ids\",\n",
    "        \"out\": \"y_onehot\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"x_tok\",\n",
    "        \"out\": \"x_weighted_emb\",\n",
    "        \"name\": \"tfidf_weighted\",\n",
    "        \"id\": \"my_weighted_embedder\",\n",
    "        \"embedder\": \"#my_embedder\",\n",
    "        \"tokenizer\": \"#my_tokenizer\",\n",
    "        \"vectorizer\": \"#my_tfidf_vectorizer\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": [\n",
    "          \"x_weighted_emb\"\n",
    "        ],\n",
    "        \"in_y\": [\n",
    "          \"y_onehot\"\n",
    "        ],\n",
    "        \"out\": [\n",
    "          \"y_pred_probas\"\n",
    "        ],\n",
    "        \"main\": True,\n",
    "        \"name\": \"keras_classification_model\",\n",
    "        \"save_path\": \"./cnn_model_v3\",\n",
    "        \"load_path\": \"./cnn_model_v3\",\n",
    "        \"embedding_size\": \"#my_embedder.dim\",\n",
    "        \"n_classes\": \"#classes_vocab.len\",\n",
    "        \"kernel_sizes_cnn\": [\n",
    "          1,\n",
    "          2,\n",
    "          3\n",
    "        ],\n",
    "        \"filters_cnn\": 256,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"learning_rate_decay\": 0.1,\n",
    "        \"loss\": \"categorical_crossentropy\",\n",
    "        \"text_size\": 15,\n",
    "        \"coef_reg_cnn\": 1e-4,\n",
    "        \"coef_reg_den\": 1e-4,\n",
    "        \"dropout_rate\": 0.5,\n",
    "        \"dense_size\": 100,\n",
    "        \"model_name\": \"cnn_model\"\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"y_pred_probas\",\n",
    "        \"out\": \"y_pred_ids\",\n",
    "        \"name\": \"proba2labels\",\n",
    "        \"max_proba\": True\n",
    "      },\n",
    "      {\n",
    "        \"in\": \"y_pred_ids\",\n",
    "        \"out\": \"y_pred_labels\",\n",
    "        \"ref\": \"classes_vocab\"\n",
    "      }\n",
    "    ],\n",
    "    \"out\": [\n",
    "      \"y_pred_labels\"\n",
    "    ]\n",
    "  },\n",
    "  \"train\": {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"metrics\": [\n",
    "      \"sets_accuracy\",\n",
    "      \"f1_macro\",\n",
    "      {\n",
    "        \"name\": \"roc_auc\",\n",
    "        \"inputs\": [\"y_onehot\", \"y_pred_probas\"]\n",
    "      }\n",
    "    ],\n",
    "    \"show_examples\": False,\n",
    "    \"validate_best\": True,\n",
    "    \"test_best\": False\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:21:51.994 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find snips/valid.csv file\n",
      "2018-10-29 16:21:51.995 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find snips/test.csv file\n",
      "2018-10-29 16:21:51.996 INFO in 'deeppavlov.dataset_iterators.basic_classification_iterator'['basic_classification_iterator'] at line 73: Splitting field <<train>> to new fields <<['train', 'valid']>>\n",
      "2018-10-29 16:21:52.1 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n",
      "2018-10-29 16:21:52.7 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 86: [saving vocabulary to snips/classes.dict]\n",
      "2018-10-29 16:21:52.37 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 217: Cannot load model from tfidf_v2.pkl\n",
      "2018-10-29 16:21:52.38 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 164: Initializing model sklearn.feature_extraction.text:TfidfVectorizer from scratch\n",
      "2018-10-29 16:21:52.65 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 107: Fitting model sklearn.feature_extraction.text:TfidfVectorizer\n",
      "2018-10-29 16:21:52.177 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 239: Saving model to tfidf_v2.pkl\n",
      "2018-10-29 16:21:52.196 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 52: [loading fastText embeddings from `wiki.en.bin`]\n",
      "2018-10-29 16:22:49.632 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 242: [initializing `KerasClassificationModel` from scratch as cnn_model]\n",
      "2018-10-29 16:22:49.981 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 256)      77056       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 256)      153856      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 256)      230656      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 256)      1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 256)      1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 256)      1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 256)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 256)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 256)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7)            28          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 542,675\n",
      "Trainable params: 540,925\n",
      "Non-trainable params: 1,750\n",
      "__________________________________________________________________________________________________\n",
      "2018-10-29 16:26:46.453 INFO in 'deeppavlov.core.commands.train'['train'] at line 545: Saving model\n",
      "2018-10-29 16:26:46.453 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 356: [saving model to cnn_model_v3_opt.json]\n",
      "2018-10-29 16:26:47.136 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n",
      "2018-10-29 16:26:47.153 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.feature_extraction.text:TfidfVectorizer from tfidf_v2.pkl\n",
      "2018-10-29 16:26:47.170 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2018-10-29 16:26:47.171 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2018-10-29 16:26:47.172 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 52: [loading fastText embeddings from `wiki.en.bin`]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:27:42.532 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 272: [initializing `KerasClassificationModel` from saved]\n",
      "2018-10-29 16:27:42.948 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 282: [loading weights from cnn_model_v3.h5]\n",
      "2018-10-29 16:27:43.134 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 256)      77056       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 256)      153856      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 256)      230656      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 256)      1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 256)      1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 256)      1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 256)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 256)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 256)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7)            28          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 542,675\n",
      "Trainable params: 540,925\n",
      "Non-trainable params: 1,750\n",
      "__________________________________________________________________________________________________\n",
      "2018-10-29 16:27:43.138 INFO in 'deeppavlov.core.commands.train'['train'] at line 221: Testing the best saved model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.961, \"f1_macro\": 0.9607, \"roc_auc\": 0.9974}, \"time_spent\": \"0:00:03\"}}\n"
     ]
    }
   ],
   "source": [
    "# we can train and evaluate model from config\n",
    "m = train_evaluate_model_from_config(cnn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:27:46.76 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from snips/classes.dict]\n",
      "2018-10-29 16:27:46.88 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 201: Loading model sklearn.feature_extraction.text:TfidfVectorizer from tfidf_v2.pkl\n",
      "2018-10-29 16:27:46.106 INFO in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 208: Model sklearn.feature_extraction.textTfidfVectorizer loaded  with parameters\n",
      "2018-10-29 16:27:46.106 WARNING in 'deeppavlov.models.sklearn.sklearn_component'['sklearn_component'] at line 214: Fitting of loaded model can not be continued. Model can be fitted from scratch.If one needs to continue fitting, please, look at `warm_start` parameter\n",
      "2018-10-29 16:27:46.108 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 52: [loading fastText embeddings from `wiki.en.bin`]\n",
      "2018-10-29 16:28:41.707 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 272: [initializing `KerasClassificationModel` from saved]\n",
      "2018-10-29 16:28:42.78 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 282: [loading weights from cnn_model_v3.h5]\n",
      "2018-10-29 16:28:42.299 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 256)      77056       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 256)      153856      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 256)      230656      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 256)      1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 256)      1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 256)      1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 256)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 256)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 256)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7)            28          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 542,675\n",
      "Trainable params: 540,925\n",
      "Non-trainable params: 1,750\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# or we can just load pre-trained model (conicides with what we did above)\n",
    "m = build_model_from_config(cnn_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['GetWeather']]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m([\"Is it freezing in Offerman, California?\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: pre-trained CNN model in DeepPavlov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download model files (`wiki.en.bin` 8Gb embeddings):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! python -m deeppavlov download intents_snips_big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate metrics on validation set (no test set provided):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! python -m deeppavlov evaluate intents_snips_big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or one can use model from python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import deeppavlov\n",
    "from deeppavlov.core.commands.infer import build_model_from_config\n",
    "from deeppavlov.download import deep_download\n",
    "\n",
    "config_path = Path(deeppavlov.__file__).parent.joinpath('configs/classifiers/intents_snips_big.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:29:11.234 INFO in 'deeppavlov.download'['download'] at line 112: Downloading...\n",
      "2018-10-29 16:29:11.240 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 208: Starting new HTTP connection (1): files.deeppavlov.ai\n",
      "2018-10-29 16:29:11.273 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 396: http://files.deeppavlov.ai:80 \"GET /datasets/snips_intents/train.csv HTTP/1.1\" 200 980824\n",
      "2018-10-29 16:29:11.275 INFO in 'deeppavlov.core.data.utils'['utils'] at line 59: Downloading from http://files.deeppavlov.ai/datasets/snips_intents/train.csv to /home/dilyara/Documents/GitHub/reserve/DeepPavlov/download/snips/train.csv\n",
      "100%|██████████| 981k/981k [00:00<00:00, 21.7MB/s]\n",
      "2018-10-29 16:29:11.326 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 208: Starting new HTTP connection (1): files.deeppavlov.ai\n",
      "2018-10-29 16:29:11.365 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 396: http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/classifiers/intents_snips_v8.tar.gz HTTP/1.1\" 200 2018386\n",
      "2018-10-29 16:29:11.366 INFO in 'deeppavlov.core.data.utils'['utils'] at line 59: Downloading from http://files.deeppavlov.ai/deeppavlov_data/classifiers/intents_snips_v8.tar.gz to /home/dilyara/Documents/GitHub/reserve/DeepPavlov/download/intents_snips_v8.tar.gz\n",
      "100%|██████████| 2.02M/2.02M [00:00<00:00, 39.7MB/s]\n",
      "2018-10-29 16:29:11.418 INFO in 'deeppavlov.core.data.utils'['utils'] at line 197: Extracting /home/dilyara/Documents/GitHub/reserve/DeepPavlov/download/intents_snips_v8.tar.gz archive into /home/dilyara/Documents/GitHub/reserve/DeepPavlov/download/classifiers\n",
      "2018-10-29 16:29:11.438 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 208: Starting new HTTP connection (1): files.deeppavlov.ai\n",
      "2018-10-29 16:29:11.460 DEBUG in 'urllib3.connectionpool'['connectionpool'] at line 396: http://files.deeppavlov.ai:80 \"GET /deeppavlov_data/embeddings/wiki.en.bin HTTP/1.1\" 200 8493673445\n",
      "2018-10-29 16:29:11.461 INFO in 'deeppavlov.core.data.utils'['utils'] at line 59: Downloading from http://files.deeppavlov.ai/deeppavlov_data/embeddings/wiki.en.bin to /home/dilyara/Documents/GitHub/reserve/DeepPavlov/download/embeddings/wiki.en.bin\n",
      "100%|██████████| 8.49G/8.49G [02:03<00:00, 68.8MB/s]\n",
      "2018-10-29 16:31:14.865 INFO in 'deeppavlov.download'['download'] at line 114: \n",
      "Download successful!\n"
     ]
    }
   ],
   "source": [
    "# let's download all the required data - model files, embeddings, vocabularies\n",
    "deep_download(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:32:41.595 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from /home/dilyara/Documents/GitHub/reserve/DeepPavlov/download/classifiers/intents_snips_v8/classes.dict]\n",
      "[nltk_data] Downloading package punkt to /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     /home/dilyara/nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n",
      "2018-10-29 16:32:42.519 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 52: [loading fastText embeddings from `/home/dilyara/Documents/GitHub/reserve/DeepPavlov/download/embeddings/wiki.en.bin`]\n",
      "Using TensorFlow backend.\n",
      "2018-10-29 16:33:04.354 DEBUG in 'matplotlib'['__init__'] at line 415: $HOME=/home/dilyara\n",
      "2018-10-29 16:33:04.355 DEBUG in 'matplotlib'['__init__'] at line 415: CONFIGDIR=/home/dilyara/.config/matplotlib\n",
      "2018-10-29 16:33:04.356 DEBUG in 'matplotlib'['__init__'] at line 415: matplotlib data path: /home/dilyara/anaconda3/envs/deep36_reserve/lib/python3.6/site-packages/matplotlib/mpl-data\n",
      "2018-10-29 16:33:04.358 DEBUG in 'matplotlib'['__init__'] at line 1085: loaded rc file /home/dilyara/anaconda3/envs/deep36_reserve/lib/python3.6/site-packages/matplotlib/mpl-data/matplotlibrc\n",
      "2018-10-29 16:33:04.360 DEBUG in 'matplotlib'['__init__'] at line 1794: matplotlib version 3.0.0\n",
      "2018-10-29 16:33:04.360 DEBUG in 'matplotlib'['__init__'] at line 1795: interactive is False\n",
      "2018-10-29 16:33:04.361 DEBUG in 'matplotlib'['__init__'] at line 1796: platform is linux\n",
      "2018-10-29 16:33:04.361 DEBUG in 'matplotlib'['__init__'] at line 1797: loaded modules: ['builtins', 'sys', '_frozen_importlib', '_imp', '_warnings', '_thread', '_weakref', '_frozen_importlib_external', '_io', 'marshal', 'posix', 'zipimport', 'encodings', 'codecs', '_codecs', 'encodings.aliases', 'encodings.utf_8', '_signal', '__main__', 'encodings.latin_1', 'io', 'abc', '_weakrefset', '_bootlocale', '_locale', 'site', 'os', 'errno', 'stat', '_stat', 'posixpath', 'genericpath', 'os.path', '_collections_abc', '_sitebuiltins', 'sysconfig', '_sysconfigdata_m_linux_x86_64-linux-gnu', 'types', 'functools', '_functools', 'collections', 'operator', '_operator', 'keyword', 'heapq', '_heapq', 'itertools', 'reprlib', '_collections', 'weakref', 'collections.abc', 'importlib', 'importlib._bootstrap', 'importlib._bootstrap_external', 'warnings', 'importlib.util', 'importlib.abc', 'importlib.machinery', 'contextlib', 'mpl_toolkits', 'google', 'sphinxcontrib', 'encodings.cp437', 'runpy', 'pkgutil', 'ipykernel', 'ipykernel._version', 'ipykernel.connect', '__future__', 'json', 'json.decoder', 're', 'enum', 'sre_compile', '_sre', 'sre_parse', 'sre_constants', 'copyreg', 'json.scanner', '_json', 'json.encoder', 'subprocess', 'time', 'signal', '_posixsubprocess', 'select', 'selectors', 'math', 'threading', 'traceback', 'linecache', 'tokenize', 'token', 'IPython', 'IPython.core', 'IPython.core.getipython', 'IPython.core.release', 'IPython.core.application', 'atexit', 'copy', 'glob', 'fnmatch', 'logging', 'string', '_string', 'shutil', 'zlib', 'bz2', '_compression', '_bz2', 'lzma', '_lzma', 'pwd', 'grp', 'traitlets', 'traitlets.traitlets', 'inspect', 'ast', '_ast', 'dis', 'opcode', '_opcode', 'six', 'struct', '_struct', 'traitlets.utils', 'traitlets.utils.getargspec', 'traitlets.utils.importstring', 'ipython_genutils', 'ipython_genutils._version', 'ipython_genutils.py3compat', 'ipython_genutils.encoding', 'locale', 'platform', 'traitlets.utils.sentinel', 'traitlets.utils.bunch', 'traitlets._version', 'traitlets.config', 'traitlets.config.application', 'decorator', 'traitlets.config.configurable', 'traitlets.config.loader', 'argparse', 'textwrap', 'gettext', 'ipython_genutils.path', 'random', 'hashlib', '_hashlib', '_blake2', '_sha3', 'bisect', '_bisect', '_random', 'ipython_genutils.text', 'ipython_genutils.importstring', 'IPython.core.crashhandler', 'pprint', 'IPython.core.ultratb', 'pydoc', 'urllib', 'urllib.parse', 'IPython.core.debugger', 'bdb', 'IPython.utils', 'IPython.utils.PyColorize', 'IPython.utils.coloransi', 'IPython.utils.ipstruct', 'IPython.utils.colorable', 'pygments', 'pygments.util', 'IPython.utils.py3compat', 'IPython.utils.encoding', 'IPython.core.excolors', 'IPython.testing', 'IPython.testing.skipdoctest', 'pdb', 'cmd', 'code', 'codeop', 'IPython.core.display_trap', 'IPython.utils.path', 'IPython.utils.process', 'IPython.utils._process_posix', 'pexpect', 'pexpect.exceptions', 'pexpect.utils', 'pexpect.expect', 'pexpect.pty_spawn', 'pty', 'tty', 'termios', 'ptyprocess', 'ptyprocess.ptyprocess', 'fcntl', 'resource', 'ptyprocess.util', 'pexpect.spawnbase', 'pexpect.run', 'IPython.utils._process_common', 'shlex', 'IPython.utils.decorators', 'IPython.utils.data', 'IPython.utils.terminal', 'IPython.utils.sysinfo', 'IPython.utils._sysinfo', 'IPython.core.profiledir', 'IPython.paths', 'tempfile', 'IPython.utils.importstring', 'IPython.terminal', 'IPython.terminal.embed', 'IPython.core.compilerop', 'IPython.core.magic_arguments', 'IPython.core.error', 'IPython.utils.text', 'pathlib', 'ntpath', 'IPython.core.magic', 'getopt', 'IPython.core.oinspect', 'IPython.core.page', 'IPython.core.display', 'binascii', 'mimetypes', 'IPython.lib', 'IPython.lib.security', 'getpass', 'IPython.lib.pretty', 'datetime', '_datetime', 'IPython.utils.openpy', 'IPython.utils.dir2', 'IPython.utils.wildcard', 'pygments.lexers', 'pygments.lexers._mapping', 'pygments.modeline', 'pygments.plugin', 'pygments.lexers.python', 'pygments.lexer', 'pygments.filter', 'pygments.filters', 'pygments.token', 'pygments.regexopt', 'pygments.unistring', 'pygments.formatters', 'pygments.formatters._mapping', 'pygments.formatters.html', 'pygments.formatter', 'pygments.styles', 'IPython.core.inputtransformer2', 'typing', 'typing.io', 'typing.re', 'IPython.core.interactiveshell', 'asyncio', 'asyncio.base_events', 'concurrent', 'concurrent.futures', 'concurrent.futures._base', 'concurrent.futures.process', 'queue', 'multiprocessing', 'multiprocessing.context', 'multiprocessing.process', 'multiprocessing.reduction', 'pickle', '_compat_pickle', '_pickle', 'socket', '_socket', 'array', '__mp_main__', 'multiprocessing.connection', '_multiprocessing', 'multiprocessing.util', 'concurrent.futures.thread', 'asyncio.compat', 'asyncio.coroutines', 'asyncio.constants', 'asyncio.events', 'asyncio.base_futures', 'asyncio.log', 'asyncio.futures', 'asyncio.base_tasks', '_asyncio', 'asyncio.tasks', 'asyncio.locks', 'asyncio.protocols', 'asyncio.queues', 'asyncio.streams', 'asyncio.subprocess', 'asyncio.transports', 'asyncio.unix_events', 'asyncio.base_subprocess', 'asyncio.selector_events', 'ssl', 'ipaddress', '_ssl', 'base64', 'asyncio.sslproto', 'pickleshare', 'IPython.core.prefilter', 'IPython.core.autocall', 'IPython.core.macro', 'IPython.core.splitinput', 'IPython.core.alias', 'IPython.core.builtin_trap', 'IPython.core.events', 'backcall', 'backcall.backcall', 'IPython.core.displayhook', 'IPython.core.displaypub', 'IPython.core.extensions', 'IPython.core.formatters', 'IPython.utils.sentinel', 'IPython.core.history', 'sqlite3', 'sqlite3.dbapi2', '_sqlite3', 'IPython.core.logger', 'IPython.core.payload', 'IPython.core.usage', 'IPython.display', 'IPython.lib.display', 'html', 'html.entities', 'IPython.utils.io', 'IPython.utils.capture', 'IPython.utils.strdispatch', 'IPython.core.hooks', 'IPython.utils.syspathcontext', 'IPython.utils.tempdir', 'IPython.utils.contexts', 'IPython.core.async_helpers', 'IPython.terminal.interactiveshell', 'prompt_toolkit', 'prompt_toolkit.application', 'prompt_toolkit.application.application', 'prompt_toolkit.buffer', 'prompt_toolkit.application.current', 'prompt_toolkit.eventloop', 'prompt_toolkit.eventloop.base', 'prompt_toolkit.log', 'prompt_toolkit.eventloop.coroutine', 'prompt_toolkit.eventloop.defaults', 'prompt_toolkit.utils', 'six.moves', 'wcwidth', 'wcwidth.wcwidth', 'wcwidth.table_wide', 'wcwidth.table_zero', 'prompt_toolkit.cache', 'prompt_toolkit.eventloop.future', 'prompt_toolkit.eventloop.context', 'prompt_toolkit.eventloop.async_generator', 'six.moves.queue', 'prompt_toolkit.eventloop.event', 'prompt_toolkit.application.run_in_terminal', 'prompt_toolkit.auto_suggest', 'prompt_toolkit.filters', 'prompt_toolkit.filters.base', 'prompt_toolkit.filters.app', 'prompt_toolkit.enums', 'prompt_toolkit.filters.utils', 'prompt_toolkit.filters.cli', 'prompt_toolkit.clipboard', 'prompt_toolkit.clipboard.base', 'prompt_toolkit.selection', 'prompt_toolkit.clipboard.in_memory', 'prompt_toolkit.completion', 'prompt_toolkit.completion.base', 'prompt_toolkit.completion.filesystem', 'prompt_toolkit.completion.word_completer', 'prompt_toolkit.document', 'prompt_toolkit.history', 'prompt_toolkit.search', 'prompt_toolkit.key_binding', 'prompt_toolkit.key_binding.key_bindings', 'prompt_toolkit.keys', 'prompt_toolkit.key_binding.vi_state', 'prompt_toolkit.validation', 'prompt_toolkit.input', 'prompt_toolkit.input.base', 'prompt_toolkit.input.defaults', 'prompt_toolkit.input.typeahead', 'prompt_toolkit.key_binding.bindings', 'prompt_toolkit.key_binding.bindings.page_navigation', 'prompt_toolkit.key_binding.bindings.scroll', 'prompt_toolkit.key_binding.defaults', 'prompt_toolkit.key_binding.bindings.basic', 'prompt_toolkit.key_binding.key_processor', 'prompt_toolkit.key_binding.bindings.named_commands', 'prompt_toolkit.key_binding.bindings.completion', 'prompt_toolkit.key_binding.bindings.emacs', 'prompt_toolkit.key_binding.bindings.vi', 'prompt_toolkit.input.vt100_parser', 'prompt_toolkit.input.ansi_escape_sequences', 'prompt_toolkit.key_binding.digraphs', 'prompt_toolkit.key_binding.bindings.mouse', 'prompt_toolkit.layout', 'prompt_toolkit.layout.containers', 'prompt_toolkit.layout.controls', 'prompt_toolkit.formatted_text', 'prompt_toolkit.formatted_text.base', 'prompt_toolkit.formatted_text.html', 'xml', 'xml.dom', 'xml.dom.domreg', 'xml.dom.minidom', 'xml.dom.minicompat', 'xml.dom.xmlbuilder', 'xml.dom.NodeFilter', 'prompt_toolkit.formatted_text.ansi', 'prompt_toolkit.output', 'prompt_toolkit.output.base', 'prompt_toolkit.layout.screen', 'prompt_toolkit.output.defaults', 'prompt_toolkit.output.color_depth', 'prompt_toolkit.output.vt100', 'prompt_toolkit.styles', 'prompt_toolkit.styles.base', 'prompt_toolkit.styles.defaults', 'prompt_toolkit.styles.style', 'prompt_toolkit.styles.named_colors', 'prompt_toolkit.styles.pygments', 'prompt_toolkit.styles.style_transformation', 'colorsys', 'prompt_toolkit.formatted_text.pygments', 'prompt_toolkit.formatted_text.utils', 'prompt_toolkit.lexers', 'prompt_toolkit.lexers.base', 'prompt_toolkit.lexers.pygments', 'prompt_toolkit.mouse_events', 'prompt_toolkit.layout.processors', 'prompt_toolkit.layout.utils', 'prompt_toolkit.layout.dimension', 'prompt_toolkit.layout.margins', 'prompt_toolkit.layout.layout', 'prompt_toolkit.layout.menus', 'prompt_toolkit.renderer', 'prompt_toolkit.layout.mouse_handlers', 'prompt_toolkit.key_binding.bindings.cpr', 'prompt_toolkit.key_binding.emacs_state', 'prompt_toolkit.layout.dummy', 'prompt_toolkit.application.dummy', 'prompt_toolkit.shortcuts', 'prompt_toolkit.shortcuts.dialogs', 'prompt_toolkit.key_binding.bindings.focus', 'prompt_toolkit.widgets', 'prompt_toolkit.widgets.base', 'prompt_toolkit.widgets.toolbars', 'prompt_toolkit.widgets.dialogs', 'prompt_toolkit.widgets.menus', 'prompt_toolkit.shortcuts.prompt', 'prompt_toolkit.key_binding.bindings.auto_suggest', 'prompt_toolkit.key_binding.bindings.open_in_editor', 'prompt_toolkit.shortcuts.utils', 'prompt_toolkit.shortcuts.progress_bar', 'prompt_toolkit.shortcuts.progress_bar.base', 'prompt_toolkit.shortcuts.progress_bar.formatters', 'prompt_toolkit.patch_stdout', 'pygments.style', 'IPython.terminal.debugger', 'IPython.core.completer', 'unicodedata', 'IPython.core.latex_symbols', 'IPython.utils.generics', 'simplegeneric', 'jedi', 'jedi.api', 'parso', 'parso.parser', 'parso.tree', 'parso._compatibility', 'parso.pgen2', 'parso.pgen2.generator', 'parso.pgen2.grammar_parser', 'parso.python', 'parso.python.tokenize', 'parso.python.token', 'parso.utils', 'parso.grammar', 'parso.python.diff', 'difflib', 'parso.python.parser', 'parso.python.tree', 'parso.python.prefix', 'parso.cache', 'gc', 'parso.python.errors', 'parso.normalizer', 'parso.python.pep8', 'jedi._compatibility', 'jedi.parser_utils', 'jedi.debug', 'jedi.settings', 'jedi.cache', 'jedi.api.classes', 'jedi.evaluate', 'jedi.evaluate.utils', 'jedi.evaluate.imports', 'jedi.evaluate.sys_path', 'jedi.evaluate.cache', 'jedi.evaluate.base_context', 'jedi.common', 'jedi.common.context', 'jedi.evaluate.helpers', 'jedi.common.utils', 'jedi.evaluate.compiled', 'jedi.evaluate.compiled.context', 'jedi.evaluate.filters', 'jedi.evaluate.flow_analysis', 'jedi.evaluate.recursion', 'jedi.evaluate.lazy_context', 'jedi.evaluate.compiled.access', 'jedi.evaluate.compiled.getattr_static', 'jedi.evaluate.compiled.fake', 'jedi.evaluate.analysis', 'jedi.evaluate.context', 'jedi.evaluate.context.module', 'jedi.evaluate.context.klass', 'jedi.evaluate.context.function', 'jedi.evaluate.docstrings', 'jedi.evaluate.pep0484', 'jedi.evaluate.arguments', 'jedi.evaluate.context.iterable', 'jedi.evaluate.param', 'jedi.evaluate.context.asynchronous', 'jedi.evaluate.parser_cache', 'jedi.evaluate.context.instance', 'jedi.evaluate.syntax_tree', 'jedi.evaluate.finder', 'jedi.api.keywords', 'pydoc_data', 'pydoc_data.topics', 'jedi.api.interpreter', 'jedi.evaluate.compiled.mixed', 'jedi.api.helpers', 'jedi.api.completion', 'jedi.api.environment', 'filecmp', 'jedi.evaluate.compiled.subprocess', 'jedi.evaluate.compiled.subprocess.functions', 'jedi.api.exceptions', 'jedi.api.project', 'jedi.evaluate.usages', 'IPython.terminal.ptutils', 'IPython.terminal.shortcuts', 'IPython.terminal.magics', 'IPython.lib.clipboard', 'IPython.terminal.pt_inputhooks', 'IPython.terminal.prompts', 'IPython.terminal.ipapp', 'IPython.core.magics', 'IPython.core.magics.auto', 'IPython.core.magics.basic', 'IPython.core.magics.code', 'urllib.request', 'email', 'http', 'http.client', 'email.parser', 'email.feedparser', 'email.errors', 'email._policybase', 'email.header', 'email.quoprimime', 'email.base64mime', 'email.charset', 'email.encoders', 'quopri', 'email.utils', 'email._parseaddr', 'calendar', 'email.message', 'uu', 'email._encoded_words', 'email.iterators', 'urllib.error', 'urllib.response', 'IPython.core.magics.config', 'IPython.core.magics.display', 'IPython.core.magics.execution', 'timeit', 'cProfile', '_lsprof', 'profile', 'optparse', 'pstats', 'IPython.utils.module_paths', 'IPython.utils.timing', 'IPython.core.magics.extension', 'IPython.core.magics.history', 'IPython.core.magics.logging', 'IPython.core.magics.namespace', 'IPython.core.magics.osm', 'IPython.core.magics.pylab', 'IPython.core.pylabtools', 'IPython.core.magics.script', 'IPython.lib.backgroundjobs', 'IPython.core.shellapp', 'IPython.extensions', 'IPython.extensions.storemagic', 'IPython.utils.frame', 'jupyter_client', 'jupyter_client._version', 'jupyter_client.connect', 'zmq', 'ctypes', '_ctypes', 'ctypes._endian', 'zmq.backend', 'zmq.backend.select', 'zmq.backend.cython', 'cython_runtime', 'zmq.backend.cython.constants', '_cython_0_28_5', 'zmq.backend.cython.error', 'zmq.backend.cython.message', 'zmq.error', 'zmq.backend.cython.context', 'zmq.backend.cython.socket', 'zmq.backend.cython.utils', 'zmq.backend.cython._poll', 'zmq.backend.cython._version', 'zmq.backend.cython._device', 'zmq.sugar', 'zmq.sugar.constants', 'zmq.utils', 'zmq.utils.constant_names', 'zmq.sugar.context', 'zmq.sugar.attrsettr', 'zmq.sugar.socket', 'zmq.sugar.poll', 'zmq.utils.jsonapi', 'zmq.utils.strtypes', 'zmq.sugar.frame', 'zmq.sugar.tracker', 'zmq.sugar.version', 'zmq.sugar.stopwatch', 'jupyter_client.localinterfaces', 'jupyter_core', 'jupyter_core.version', 'jupyter_core.paths', 'jupyter_client.launcher', 'traitlets.log', 'jupyter_client.client', 'jupyter_client.channels', 'jupyter_client.channelsabc', 'jupyter_client.clientabc', 'jupyter_client.manager', 'jupyter_client.kernelspec', 'jupyter_client.managerabc', 'jupyter_client.blocking', 'jupyter_client.blocking.client', 'jupyter_client.blocking.channels', 'jupyter_client.multikernelmanager', 'uuid', 'ctypes.util', 'ipykernel.kernelapp', 'tornado', 'tornado.ioloop', 'numbers', 'tornado.concurrent', 'tornado.log', 'logging.handlers', 'tornado.escape', 'tornado.util', 'tornado.speedups', 'curses', '_curses', 'tornado.stack_context', 'tornado.platform', 'tornado.platform.auto', 'tornado.platform.posix', 'tornado.platform.common', 'tornado.platform.interface', 'zmq.eventloop', 'zmq.eventloop.ioloop', 'tornado.platform.asyncio', 'tornado.gen', 'zmq.eventloop.zmqstream', 'ipykernel.iostream', 'imp', 'jupyter_client.session', 'hmac', 'jupyter_client.jsonutil', 'dateutil', 'dateutil._version', 'dateutil.parser', 'dateutil.parser._parser', 'decimal', '_decimal', 'dateutil.relativedelta', 'dateutil._common', 'dateutil.tz', 'dateutil.tz.tz', 'dateutil.tz._common', 'dateutil.tz._factories', 'dateutil.parser.isoparser', '_strptime', 'jupyter_client.adapter', 'ipykernel.heartbeat', 'ipykernel.ipkernel', 'IPython.utils.tokenutil', 'ipykernel.comm', 'ipykernel.comm.manager', 'ipykernel.comm.comm', 'ipykernel.kernelbase', 'ipykernel.jsonutil', 'ipykernel.zmqshell', 'IPython.core.payloadpage', 'ipykernel.displayhook', 'ipykernel.parentpoller', 'faulthandler', 'ipykernel.datapub', 'ipykernel.serialize', 'ipykernel.pickleutil', 'ipykernel.codeutil', 'IPython.core.completerlib', 'storemagic', 'deeppavlov', 'deeppavlov.core', 'deeppavlov.core.data', 'deeppavlov.core.data.utils', 'gzip', 'secrets', 'tarfile', 'zipfile', 'numpy', 'numpy._globals', 'numpy.__config__', 'numpy.version', 'numpy._import_tools', 'numpy.add_newdocs', 'numpy.lib', 'numpy.lib.info', 'numpy.lib.type_check', 'numpy.core', 'numpy.core.info', 'numpy.core.multiarray', 'numpy.core.umath', 'numpy.core._internal', 'numpy.compat', 'numpy.compat._inspect', 'numpy.compat.py3k', 'numpy.core.numerictypes', 'numpy.core.numeric', 'numpy.core.fromnumeric', 'numpy.core._methods', 'numpy.core.arrayprint', 'numpy.core.defchararray', 'numpy.core.records', 'numpy.core.memmap', 'numpy.core.function_base', 'numpy.core.machar', 'numpy.core.getlimits', 'numpy.core.shape_base', 'numpy.core.einsumfunc', 'numpy.testing', 'unittest', 'unittest.result', 'unittest.util', 'unittest.case', 'unittest.suite', 'unittest.loader', 'unittest.main', 'unittest.runner', 'unittest.signals', 'numpy.testing.decorators', 'numpy.testing.nose_tools', 'numpy.testing.nose_tools.decorators', 'numpy.testing.nose_tools.utils', 'numpy.lib.utils', 'numpy.testing.nosetester', 'numpy.testing.nose_tools.nosetester', 'numpy.testing.utils', 'numpy.lib.ufunclike', 'numpy.lib.index_tricks', 'numpy.lib.function_base', 'numpy.lib.twodim_base', 'numpy.matrixlib', 'numpy.matrixlib.defmatrix', 'numpy.lib.stride_tricks', 'numpy.lib.mixins', 'numpy.lib.nanfunctions', 'numpy.lib.shape_base', 'numpy.lib.scimath', 'numpy.lib.polynomial', 'numpy.linalg', 'numpy.linalg.info', 'numpy.linalg.linalg', 'numpy.linalg.lapack_lite', 'numpy.linalg._umath_linalg', 'numpy.lib.arraysetops', 'numpy.lib.npyio', 'numpy.lib.format', 'numpy.lib._datasource', 'numpy.lib._iotools', 'numpy.lib.financial', 'numpy.lib.arrayterator', 'numpy.lib.arraypad', 'numpy.lib._version', 'numpy._distributor_init', 'numpy.fft', 'numpy.fft.info', 'numpy.fft.fftpack', 'numpy.fft.fftpack_lite', 'numpy.fft.helper', 'numpy.polynomial', 'numpy.polynomial.polynomial', 'numpy.polynomial.polyutils', 'numpy.polynomial._polybase', 'numpy.polynomial.chebyshev', 'numpy.polynomial.legendre', 'numpy.polynomial.hermite', 'numpy.polynomial.hermite_e', 'numpy.polynomial.laguerre', 'numpy.random', 'numpy.random.info', 'mtrand', 'numpy.random.mtrand', 'numpy.ctypeslib', 'numpy.ma', 'numpy.ma.core', 'numpy.ma.extras', 'requests', 'urllib3', 'urllib3.connectionpool', 'urllib3.exceptions', 'urllib3.packages', 'urllib3.packages.ssl_match_hostname', 'urllib3.packages.six', 'urllib3.packages.six.moves', 'urllib3.packages.six.moves.http_client', 'urllib3.connection', 'urllib3.util', 'urllib3.util.connection', 'urllib3.util.wait', 'urllib3.util.selectors', 'urllib3.util.request', 'urllib3.util.response', 'urllib3.util.ssl_', 'urllib3.util.timeout', 'urllib3.util.retry', 'urllib3.util.url', 'urllib3._collections', 'urllib3.request', 'urllib3.filepost', 'urllib3.fields', 'urllib3.packages.six.moves.urllib', 'urllib3.packages.six.moves.urllib.parse', 'urllib3.response', 'urllib3.poolmanager', 'chardet', 'chardet.compat', 'chardet.universaldetector', 'chardet.charsetgroupprober', 'chardet.enums', 'chardet.charsetprober', 'chardet.escprober', 'chardet.codingstatemachine', 'chardet.escsm', 'chardet.latin1prober', 'chardet.mbcsgroupprober', 'chardet.utf8prober', 'chardet.mbcssm', 'chardet.sjisprober', 'chardet.mbcharsetprober', 'chardet.chardistribution', 'chardet.euctwfreq', 'chardet.euckrfreq', 'chardet.gb2312freq', 'chardet.big5freq', 'chardet.jisfreq', 'chardet.jpcntx', 'chardet.eucjpprober', 'chardet.gb2312prober', 'chardet.euckrprober', 'chardet.cp949prober', 'chardet.big5prober', 'chardet.euctwprober', 'chardet.sbcsgroupprober', 'chardet.sbcharsetprober', 'chardet.langcyrillicmodel', 'chardet.langgreekmodel', 'chardet.langbulgarianmodel', 'chardet.langthaimodel', 'chardet.langhebrewmodel', 'chardet.hebrewprober', 'chardet.langturkishmodel', 'chardet.version', 'requests.exceptions', 'urllib3.contrib', 'requests.__version__', 'requests.utils', 'requests.certs', 'certifi', 'certifi.core', 'requests._internal_utils', 'requests.compat', 'http.cookiejar', 'http.cookies', 'requests.cookies', 'requests.structures', 'requests.packages', 'requests.packages.urllib3', 'requests.packages.urllib3.connectionpool', 'requests.packages.urllib3.exceptions', 'requests.packages.urllib3.packages', 'requests.packages.urllib3.packages.ssl_match_hostname', 'requests.packages.urllib3.packages.six', 'requests.packages.urllib3.packages.six.moves', 'requests.packages.urllib3.packages.six.moves.http_client', 'requests.packages.urllib3.connection', 'requests.packages.urllib3.util', 'requests.packages.urllib3.util.connection', 'requests.packages.urllib3.util.wait', 'requests.packages.urllib3.util.selectors', 'requests.packages.urllib3.util.request', 'requests.packages.urllib3.util.response', 'requests.packages.urllib3.util.ssl_', 'requests.packages.urllib3.util.timeout', 'requests.packages.urllib3.util.retry', 'requests.packages.urllib3.util.url', 'requests.packages.urllib3._collections', 'requests.packages.urllib3.request', 'requests.packages.urllib3.filepost', 'requests.packages.urllib3.fields', 'requests.packages.urllib3.packages.six.moves.urllib', 'requests.packages.urllib3.packages.six.moves.urllib.parse', 'requests.packages.urllib3.response', 'requests.packages.urllib3.poolmanager', 'requests.packages.urllib3.contrib', 'idna', 'idna.package_data', 'idna.core', 'idna.idnadata', 'idna.intranges', 'requests.packages.idna', 'requests.packages.idna.package_data', 'requests.packages.idna.core', 'requests.packages.idna.idnadata', 'requests.packages.idna.intranges', 'requests.packages.chardet', 'requests.packages.chardet.compat', 'requests.packages.chardet.universaldetector', 'requests.packages.chardet.charsetgroupprober', 'requests.packages.chardet.enums', 'requests.packages.chardet.charsetprober', 'requests.packages.chardet.escprober', 'requests.packages.chardet.codingstatemachine', 'requests.packages.chardet.escsm', 'requests.packages.chardet.latin1prober', 'requests.packages.chardet.mbcsgroupprober', 'requests.packages.chardet.utf8prober', 'requests.packages.chardet.mbcssm', 'requests.packages.chardet.sjisprober', 'requests.packages.chardet.mbcharsetprober', 'requests.packages.chardet.chardistribution', 'requests.packages.chardet.euctwfreq', 'requests.packages.chardet.euckrfreq', 'requests.packages.chardet.gb2312freq', 'requests.packages.chardet.big5freq', 'requests.packages.chardet.jisfreq', 'requests.packages.chardet.jpcntx', 'requests.packages.chardet.eucjpprober', 'requests.packages.chardet.gb2312prober', 'requests.packages.chardet.euckrprober', 'requests.packages.chardet.cp949prober', 'requests.packages.chardet.big5prober', 'requests.packages.chardet.euctwprober', 'requests.packages.chardet.sbcsgroupprober', 'requests.packages.chardet.sbcharsetprober', 'requests.packages.chardet.langcyrillicmodel', 'requests.packages.chardet.langgreekmodel', 'requests.packages.chardet.langbulgarianmodel', 'requests.packages.chardet.langthaimodel', 'requests.packages.chardet.langhebrewmodel', 'requests.packages.chardet.hebrewprober', 'requests.packages.chardet.langturkishmodel', 'requests.packages.chardet.version', 'requests.models', 'encodings.idna', 'stringprep', 'requests.hooks', 'requests.auth', 'requests.status_codes', 'requests.api', 'requests.sessions', 'requests.adapters', 'tqdm', 'tqdm._tqdm', 'tqdm._utils', 'tqdm._monitor', 'multiprocessing.synchronize', 'tqdm._tqdm_gui', 'tqdm._tqdm_pandas', 'tqdm._main', 'tqdm._version', 'deeppavlov.core.common', 'deeppavlov.core.common.log', 'logging.config', 'socketserver', 'deeppavlov.core.commands', 'deeppavlov.core.commands.utils', 'deeppavlov.core.common.paths', 'netrc', 'deeppavlov.core.commands.infer', 'deeppavlov.core.common.chainer', 'deeppavlov.core.common.errors', 'deeppavlov.core.models', 'deeppavlov.core.models.component', 'deeppavlov.core.models.nn_model', 'deeppavlov.core.models.serializable', 'deeppavlov.core.common.file', 'deeppavlov.core.common.params', 'deeppavlov.core.common.registry', 'deeppavlov.core.commands.train', 'deeppavlov.core.common.metrics_registry', 'deeppavlov.core.data.data_fitting_iterator', 'deeppavlov.core.data.data_learning_iterator', 'deeppavlov.core.models.estimator', 'deeppavlov.download', 'deeppavlov.core.data.simple_vocab', 'deeppavlov.models', 'nltk', 'nltk.internals', 'xml.etree', 'xml.etree.cElementTree', 'xml.etree.ElementTree', 'xml.etree.ElementPath', 'pyexpat.errors', 'pyexpat.model', 'pyexpat', '_elementtree', 'nltk.compat', 'fractions', 'nltk.collocations', 'nltk.probability', 'nltk.util', 'six.moves.urllib', 'six.moves.urllib.request', 'nltk.collections', 'nltk.metrics', 'nltk.metrics.scores', 'scipy', 'scipy._distributor_init', 'scipy.__config__', 'scipy.version', 'scipy._lib', 'scipy._lib._testutils', 'scipy._lib._version', 'scipy._lib.six', 'scipy._lib._ccallback', 'scipy._lib._ccallback_c', 'scipy.stats', 'scipy.stats.stats', 'scipy.special', 'scipy.special.sf_error', '_cython_0_28_2', 'scipy.special._ufuncs', 'scipy.special._ufuncs_cxx', 'scipy.special.basic', 'scipy.special.specfun', 'scipy.special.orthogonal', 'scipy.linalg', 'scipy.linalg.linalg_version', 'scipy.linalg.misc', 'scipy.linalg.blas', 'scipy.linalg._fblas', 'scipy.linalg.lapack', 'scipy.linalg._flapack', 'scipy._lib._util', 'scipy.linalg.basic', 'scipy.linalg.flinalg', 'scipy.linalg._flinalg', 'scipy.linalg.decomp', 'scipy.linalg.decomp_svd', 'scipy.linalg._solve_toeplitz', 'scipy.linalg.decomp_lu', 'scipy.linalg._decomp_ldl', 'scipy.linalg.decomp_cholesky', 'scipy.linalg.decomp_qr', 'scipy.linalg._decomp_qz', 'scipy.linalg.decomp_schur', 'scipy.linalg._decomp_polar', 'scipy.linalg.matfuncs', 'scipy.linalg.special_matrices', 'scipy.linalg._expm_frechet', 'scipy.linalg._matfuncs_sqrtm', 'scipy.linalg._solvers', 'scipy.linalg._procrustes', 'scipy.linalg._decomp_update', 'scipy.linalg.cython_blas', 'scipy.linalg.cython_lapack', 'scipy.linalg._sketches', 'numpy.dual', 'scipy.special._comb', 'scipy.special._logsumexp', 'scipy.special.spfun_stats', 'scipy.special._ellip_harm', 'scipy.special._ellip_harm_2', 'scipy.special.lambertw', 'scipy.special._spherical_bessel', 'scipy.stats.distributions', 'scipy.stats._distn_infrastructure', 'scipy.misc', 'scipy.misc.doccer', 'scipy.misc.common', 'scipy.interpolate', 'scipy.interpolate.interpolate', 'scipy.interpolate.fitpack', 'scipy.interpolate._fitpack_impl', 'scipy.interpolate._fitpack', 'scipy.interpolate.dfitpack', 'scipy.interpolate._bsplines', 'scipy.interpolate._bspl', 'scipy.interpolate.polyint', 'scipy.interpolate._ppoly', 'scipy.interpolate.fitpack2', 'scipy.interpolate.interpnd', 'scipy.spatial', 'scipy.spatial.kdtree', 'scipy.sparse', 'scipy.sparse.base', 'scipy._lib._numpy_compat', 'scipy.sparse.sputils', 'scipy.sparse.csr', 'scipy.sparse._sparsetools', 'scipy.sparse.compressed', 'scipy.sparse.data', 'scipy.sparse.dia', 'scipy.sparse.csc', 'scipy.sparse.lil', 'scipy.sparse._csparsetools', 'scipy.sparse.dok', 'scipy.sparse.coo', 'scipy.sparse.bsr', 'scipy.sparse.construct', 'scipy.sparse.extract', 'scipy.sparse._matrix_io', 'scipy.sparse.csgraph', 'scipy.sparse.csgraph._laplacian', 'scipy.sparse.csgraph._shortest_path', 'scipy.sparse.csgraph._validation', 'scipy.sparse.csgraph._tools', 'scipy.sparse.csgraph._traversal', 'scipy.sparse.csgraph._min_spanning_tree', 'scipy.sparse.csgraph._reordering', 'scipy.spatial.ckdtree', 'scipy.spatial.qhull', 'scipy._lib.messagestream', 'scipy.spatial._spherical_voronoi', 'numpy.matlib', 'scipy.spatial._voronoi', 'scipy.spatial.distance', 'scipy.spatial._distance_wrap', 'scipy.spatial._hausdorff', 'scipy.spatial._plotutils', 'scipy._lib.decorator', 'scipy.spatial._procrustes', 'scipy.interpolate.rbf', 'scipy.interpolate._cubic', 'scipy.interpolate.ndgriddata', 'scipy.interpolate._pade', 'scipy.stats._distr_params', 'scipy.optimize', 'scipy.optimize.optimize', 'scipy.optimize.linesearch', 'scipy.optimize.minpack2', 'scipy.optimize._minimize', 'scipy.sparse.linalg', 'scipy.sparse.linalg.isolve', 'scipy.sparse.linalg.isolve.iterative', 'scipy.sparse.linalg.isolve._iterative', 'scipy.sparse.linalg.interface', 'scipy.sparse.linalg.isolve.utils', 'scipy._lib._threadsafety', 'scipy.sparse.linalg.isolve.minres', 'scipy.sparse.linalg.isolve.lgmres', 'scipy.sparse.linalg.isolve._gcrotmk', 'scipy.sparse.linalg.isolve.lsqr', 'scipy.sparse.linalg.isolve.lsmr', 'scipy.sparse.linalg.dsolve', 'scipy.sparse.linalg.dsolve.linsolve', 'scipy.sparse.linalg.dsolve._superlu', 'scipy.sparse.linalg.dsolve._add_newdocs', 'scipy.sparse.linalg.eigen', 'scipy.sparse.linalg.eigen.arpack', 'scipy.sparse.linalg.eigen.arpack.arpack', 'scipy.sparse.linalg.eigen.arpack._arpack', 'scipy.sparse.linalg.eigen.lobpcg', 'scipy.sparse.linalg.eigen.lobpcg.lobpcg', 'scipy.sparse.linalg.matfuncs', 'scipy.sparse.linalg._onenormest', 'scipy.sparse.linalg._norm', 'scipy.sparse.linalg._expm_multiply', 'scipy.optimize._trustregion_dogleg', 'scipy.optimize._trustregion', 'scipy.optimize._trustregion_ncg', 'scipy.optimize._trustregion_krylov', 'scipy.optimize._trlib', 'scipy.optimize._trlib._trlib', 'scipy.optimize._trustregion_exact', 'scipy.optimize._trustregion_constr', 'scipy.optimize._trustregion_constr.minimize_trustregion_constr', 'scipy.optimize._differentiable_functions', 'scipy.optimize._numdiff', 'scipy.optimize._group_columns', 'scipy.optimize._hessian_update_strategy', 'scipy.optimize._constraints', 'scipy.optimize._trustregion_constr.equality_constrained_sqp', 'scipy.optimize._trustregion_constr.projections', 'scipy.optimize._trustregion_constr.qp_subproblem', 'scipy.optimize._trustregion_constr.canonical_constraint', 'scipy.optimize._trustregion_constr.tr_interior_point', 'scipy.optimize._trustregion_constr.report', 'scipy.optimize.lbfgsb', 'scipy.optimize._lbfgsb', 'scipy.optimize.tnc', 'scipy.optimize.moduleTNC', 'scipy.optimize.cobyla', 'scipy.optimize._cobyla', 'scipy.optimize.slsqp', 'scipy.optimize._slsqp', 'scipy.optimize._root', 'scipy.optimize.minpack', 'scipy.optimize._minpack', 'scipy.optimize._lsq', 'scipy.optimize._lsq.least_squares', 'scipy.optimize._lsq.trf', 'scipy.optimize._lsq.common', 'scipy.optimize._lsq.dogbox', 'scipy.optimize._lsq.lsq_linear', 'scipy.optimize._lsq.trf_linear', 'scipy.optimize._lsq.givens_elimination', 'scipy.optimize._lsq.bvls', 'scipy.optimize._spectral', 'scipy.optimize.nonlin', 'scipy.optimize.zeros', 'scipy.optimize._zeros', 'scipy.optimize.nnls', 'scipy.optimize._nnls', 'scipy.optimize._basinhopping', 'scipy.optimize._linprog', 'scipy.optimize._linprog_ip', 'scipy.optimize._remove_redundancy', 'scipy.optimize._hungarian', 'scipy.optimize._differentialevolution', 'scipy.integrate', 'scipy.integrate.quadrature', 'scipy.integrate.odepack', 'scipy.integrate._odepack', 'scipy.integrate.quadpack', 'scipy.integrate._quadpack', 'scipy.integrate._ode', 'scipy.integrate.vode', 'scipy.integrate._dop', 'scipy.integrate.lsoda', 'scipy.integrate._bvp', 'scipy.integrate._ivp', 'scipy.integrate._ivp.ivp', 'scipy.integrate._ivp.bdf', 'scipy.integrate._ivp.common', 'scipy.integrate._ivp.base', 'scipy.integrate._ivp.radau', 'scipy.integrate._ivp.rk', 'scipy.integrate._ivp.lsoda', 'scipy.stats._constants', 'scipy.stats._continuous_distns', 'scipy.stats._stats', 'scipy.stats._tukeylambda_stats', 'scipy.stats._discrete_distns', 'scipy.stats.mstats_basic', 'scipy.stats._stats_mstats_common', 'scipy.stats.morestats', 'scipy.stats.statlib', 'scipy.stats.contingency', 'scipy.stats._binned_statistic', 'scipy.stats.kde', 'scipy.stats.mvn', 'scipy.stats.mstats', 'scipy.stats.mstats_extras', 'scipy.stats._multivariate', 'nltk.metrics.confusionmatrix', 'nltk.metrics.distance', 'nltk.metrics.paice', 'nltk.metrics.segmentation', 'nltk.metrics.agreement', 'nltk.metrics.association', 'nltk.metrics.spearman', 'nltk.metrics.aline', 'nltk.decorators', 'nltk.featstruct', 'nltk.sem', 'nltk.sem.util', 'nltk.sem.evaluate', 'nltk.sem.logic', 'nltk.sem.skolemize', 'nltk.sem.lfg', 'nltk.sem.relextract', 'nltk.sem.boxer', 'nltk.sem.drt', 'tkinter', '_tkinter', 'tkinter.constants', 'six.moves.tkinter', 'tkinter.font', 'six.moves.tkinter_font', 'nltk.grammar', 'nltk.text', 'nltk.tree', 'nltk.jsontags', 'nltk.chunk', 'nltk.data', 'nltk.chunk.api', 'nltk.parse', 'nltk.parse.api', 'nltk.parse.chart', 'nltk.parse.featurechart', 'nltk.parse.earleychart', 'nltk.parse.pchart', 'nltk.parse.recursivedescent', 'nltk.parse.shiftreduce', 'nltk.parse.util', 'nltk.parse.viterbi', 'nltk.parse.dependencygraph', 'nltk.parse.projectivedependencyparser', 'nltk.parse.nonprojectivedependencyparser', 'nltk.parse.malt', 'nltk.parse.evaluate', 'nltk.parse.transitionparser', 'sklearn', 'sklearn.__check_build', 'sklearn.__check_build._check_build', 'sklearn.base', 'sklearn.externals', 'sklearn.externals.six', 'sklearn.externals.six.moves', 'sklearn.externals.six.moves.urllib_parse', 'sklearn.externals.six.moves.urllib.parse', 'sklearn.externals.six.moves.urllib_error', 'sklearn.externals.six.moves.urllib.error', 'sklearn.externals.six.moves.urllib_request', 'sklearn.externals.six.moves.urllib.request', 'sklearn.externals.six.moves.urllib_response', 'sklearn.externals.six.moves.urllib.response', 'sklearn.externals.six.moves.urllib_robotparser', 'sklearn.externals.six.moves.urllib.robotparser', 'sklearn.externals.six.moves.urllib', 'sklearn.utils', 'sklearn.utils.murmurhash', 'sklearn.utils.validation', 'sklearn.utils.fixes', 'sklearn.exceptions', 'sklearn.externals.joblib', 'sklearn.externals.joblib.memory', 'sklearn.externals.joblib.hashing', 'sklearn.externals.joblib._compat', 'sklearn.externals.joblib.func_inspect', 'sklearn.externals.joblib.logger', 'sklearn.externals.joblib.disk', 'sklearn.externals.joblib._memory_helpers', 'sklearn.externals.joblib.numpy_pickle', 'sklearn.externals.joblib.numpy_pickle_utils', 'sklearn.externals.joblib.numpy_pickle_compat', 'sklearn.externals.joblib.backports', 'distutils', 'distutils.version', 'sklearn.externals.joblib.parallel', 'sklearn.externals.joblib._multiprocessing_helpers', 'sklearn.externals.joblib.format_stack', 'sklearn.externals.joblib.my_exceptions', 'sklearn.externals.joblib._parallel_backends', 'sklearn.externals.joblib.pool', 'mmap', 'multiprocessing.pool', 'sklearn.utils.class_weight', 'sklearn.utils.deprecation', 'sklearn.datasets', 'sklearn.datasets.base', 'csv', '_csv', 'sklearn.datasets.covtype', 'sklearn.datasets.kddcup99', 'sklearn.datasets.mlcomp', 'sklearn.datasets.lfw', 'sklearn.datasets.twenty_newsgroups', 'sklearn.feature_extraction', 'sklearn.feature_extraction.dict_vectorizer', 'sklearn.feature_extraction.hashing', 'sklearn.feature_extraction._hashing', 'sklearn.feature_extraction.image', 'sklearn.feature_extraction.text', 'sklearn.preprocessing', 'sklearn.preprocessing._function_transformer', 'sklearn.preprocessing.data', 'sklearn.utils.extmath', 'sklearn.utils._logistic_sigmoid', '_cython_0_27_1', 'sklearn.utils.sparsefuncs_fast', 'sklearn.utils.sparsefuncs', 'sklearn.preprocessing.label', 'sklearn.utils.multiclass', 'sklearn.preprocessing.imputation', 'sklearn.feature_extraction.stop_words', 'sklearn.datasets.mldata', 'scipy.io', 'scipy.io.matlab', 'scipy.io.matlab.mio', 'scipy.io.matlab.miobase', 'scipy.io.matlab.byteordercodes', 'scipy.io.matlab.mio4', 'scipy.io.matlab.mio_utils', 'scipy.io.matlab.mio5', 'scipy.io.matlab.mio5_utils', 'scipy.io.matlab.streams', 'scipy.io.matlab.mio5_params', 'scipy.io.netcdf', 'scipy.io._fortran', 'scipy.io.mmio', 'scipy.io.idl', 'scipy.io.harwell_boeing', 'scipy.io.harwell_boeing.hb', 'scipy.io.harwell_boeing._fortran_format_parser', 'sklearn.datasets.samples_generator', 'sklearn.utils.random', 'sklearn.utils._random', 'sklearn.datasets.svmlight_format', 'sklearn.datasets._svmlight_format', 'sklearn.datasets.olivetti_faces', 'sklearn.datasets.species_distributions', 'sklearn.datasets.california_housing', 'sklearn.datasets.rcv1', 'sklearn.svm', 'sklearn.svm.classes', 'sklearn.svm.base', 'sklearn.svm.libsvm', 'sklearn.svm.liblinear', 'sklearn.svm.libsvm_sparse', 'sklearn.linear_model', 'sklearn.linear_model.base', 'sklearn.utils.seq_dataset', 'sklearn.linear_model.bayes', 'sklearn.linear_model.least_angle', 'sklearn.utils.arrayfuncs', 'sklearn.model_selection', 'sklearn.model_selection._split', 'sklearn.model_selection._validation', 'sklearn.utils.metaestimators', 'sklearn.metrics', 'sklearn.metrics.ranking', 'sklearn.metrics.base', 'sklearn.metrics.classification', 'sklearn.metrics.cluster', 'sklearn.metrics.cluster.supervised', 'sklearn.metrics.cluster.expected_mutual_info_fast', 'sklearn.utils.lgamma', 'sklearn.metrics.cluster.unsupervised', 'sklearn.metrics.pairwise', 'sklearn.metrics.pairwise_fast', 'sklearn.metrics.cluster.bicluster', 'sklearn.utils.linear_assignment_', 'sklearn.metrics.regression', 'sklearn.metrics.scorer', 'sklearn.model_selection._search', 'sklearn.linear_model.coordinate_descent', 'sklearn.linear_model.cd_fast', 'sklearn.linear_model.huber', 'sklearn.linear_model.sgd_fast', 'sklearn.utils.weight_vector', 'sklearn.linear_model.stochastic_gradient', 'sklearn.linear_model.ridge', 'sklearn.linear_model.sag', 'sklearn.linear_model.sag_fast', 'sklearn.linear_model.logistic', 'sklearn.utils.optimize', 'sklearn.linear_model.omp', 'sklearn.linear_model.passive_aggressive', 'sklearn.linear_model.perceptron', 'sklearn.linear_model.randomized_l1', 'sklearn.feature_selection', 'sklearn.feature_selection.univariate_selection', 'sklearn.feature_selection.base', 'sklearn.feature_selection.variance_threshold', 'sklearn.feature_selection.rfe', 'sklearn.feature_selection.from_model', 'sklearn.feature_selection.mutual_info_', 'sklearn.neighbors', 'sklearn.neighbors.ball_tree', 'sklearn.neighbors.dist_metrics', 'sklearn.neighbors.typedefs', 'sklearn.neighbors.kd_tree', 'sklearn.neighbors.graph', 'sklearn.neighbors.base', 'sklearn.neighbors.unsupervised', 'sklearn.neighbors.classification', 'sklearn.neighbors.regression', 'sklearn.neighbors.nearest_centroid', 'sklearn.neighbors.kde', 'sklearn.neighbors.approximate', 'sklearn.random_projection', 'sklearn.neighbors.lof', 'sklearn.linear_model.ransac', 'sklearn.linear_model.theil_sen', 'sklearn.svm.bounds', 'nltk.parse.bllip', 'nltk.parse.corenlp', 'nltk.tokenize', 'nltk.tokenize.casual', 'nltk.tokenize.mwe', 'nltk.tokenize.api', 'nltk.tokenize.util', 'xml.sax', 'xml.sax.xmlreader', 'xml.sax.handler', 'xml.sax._exceptions', 'xml.sax.saxutils', 'nltk.tokenize.punkt', 'nltk.tokenize.regexp', 'nltk.tokenize.repp', 'nltk.tokenize.sexpr', 'nltk.tokenize.simple', 'nltk.tokenize.texttiling', 'nltk.tokenize.toktok', 'nltk.tokenize.treebank', 'nltk.tokenize.stanford_segmenter', 'nltk.chunk.util', 'nltk.tag', 'nltk.tag.api', 'nltk.tag.util', 'nltk.tag.sequential', 'nltk.classify', 'nltk.classify.api', 'nltk.classify.megam', 'nltk.classify.weka', 'nltk.classify.naivebayes', 'nltk.classify.positivenaivebayes', 'nltk.classify.decisiontree', 'nltk.classify.rte_classify', 'nltk.classify.util', 'nltk.classify.scikitlearn', 'nltk.classify.maxent', 'nltk.classify.tadm', 'nltk.classify.senna', 'nltk.classify.textcat', 'regex', '_regex_core', '_regex', 'nltk.tag.brill', 'nltk.tbl', 'nltk.tbl.template', 'nltk.tbl.feature', 'nltk.tbl.rule', 'nltk.tbl.erroranalysis', 'nltk.tag.brill_trainer', 'nltk.tag.tnt', 'nltk.tag.hunpos', 'nltk.tag.stanford', 'nltk.tag.hmm', 'nltk.tag.senna', 'nltk.tag.mapping', 'nltk.tag.crf', 'nltk.tag.perceptron', 'nltk.chunk.regexp', 'nltk.inference', 'nltk.inference.api', 'nltk.inference.mace', 'nltk.inference.prover9', 'nltk.inference.resolution', 'nltk.inference.tableau', 'nltk.inference.discourse', 'nltk.sem.glue', 'nltk.sem.linearlogic', 'nltk.translate', 'nltk.translate.api', 'nltk.translate.ibm_model', 'nltk.translate.ibm1', 'nltk.translate.ibm2', 'nltk.translate.ibm3', 'nltk.translate.ibm4', 'nltk.translate.ibm5', 'nltk.translate.bleu_score', 'nltk.translate.ribes_score', 'nltk.translate.metrics', 'nltk.translate.stack_decoder', 'nltk.stem', 'nltk.stem.api', 'nltk.stem.regexp', 'nltk.stem.lancaster', 'nltk.stem.isri', 'nltk.stem.porter', 'nltk.stem.snowball', 'nltk.corpus', 'nltk.corpus.util', 'nltk.corpus.reader', 'nltk.corpus.reader.plaintext', 'nltk.corpus.reader.util', 'nltk.corpus.reader.api', 'nltk.corpus.reader.tagged', 'nltk.corpus.reader.timit', 'nltk.corpus.reader.cmudict', 'nltk.corpus.reader.conll', 'nltk.corpus.reader.chunked', 'nltk.corpus.reader.bracket_parse', 'nltk.corpus.reader.wordlist', 'nltk.corpus.reader.xmldocs', 'nltk.corpus.reader.ppattach', 'nltk.corpus.reader.senseval', 'nltk.corpus.reader.ieer', 'nltk.corpus.reader.sinica_treebank', 'nltk.corpus.reader.indian', 'nltk.corpus.reader.toolbox', 'nltk.toolbox', 'nltk.corpus.reader.ycoe', 'nltk.corpus.reader.rte', 'nltk.corpus.reader.string_category', 'nltk.corpus.reader.propbank', 'nltk.corpus.reader.verbnet', 'nltk.corpus.reader.bnc', 'nltk.corpus.reader.nps_chat', 'nltk.corpus.reader.wordnet', 'nltk.corpus.reader.switchboard', 'nltk.corpus.reader.dependency', 'nltk.corpus.reader.nombank', 'nltk.corpus.reader.ipipan', 'nltk.corpus.reader.pl196x', 'nltk.corpus.reader.knbc', 'nltk.corpus.reader.chasen', 'nltk.corpus.reader.childes', 'nltk.corpus.reader.aligned', 'nltk.corpus.reader.lin', 'nltk.corpus.reader.semcor', 'nltk.corpus.reader.framenet', 'nltk.corpus.reader.udhr', 'nltk.corpus.reader.sentiwordnet', 'nltk.corpus.reader.twitter', 'nltk.corpus.reader.nkjp', 'nltk.corpus.reader.crubadan', 'nltk.corpus.reader.mte', 'nltk.corpus.reader.reviews', 'nltk.corpus.reader.opinion_lexicon', 'nltk.corpus.reader.pros_cons', 'nltk.corpus.reader.categorized_sents', 'nltk.corpus.reader.comparative_sents', 'nltk.corpus.reader.panlex_lite', 'nltk.stem.util', 'nltk.stem.wordnet', 'nltk.stem.rslp', 'nltk.lazyimport', 'nltk.cluster', 'nltk.cluster.util', 'nltk.cluster.api', 'nltk.cluster.kmeans', 'nltk.cluster.gaac', 'nltk.cluster.em', 'nltk.downloader', 'tkinter.messagebox', 'tkinter.commondialog', 'six.moves.tkinter_messagebox', 'nltk.draw', 'nltk.draw.cfg', 'nltk.draw.tree', 'nltk.draw.util', 'tkinter.filedialog', 'tkinter.dialog', 'six.moves.tkinter_tkfiledialog', 'nltk.draw.table', 'nltk.draw.dispersion', 'six.moves.urllib.error', 'nltk.ccg', 'nltk.ccg.combinator', 'nltk.ccg.api', 'nltk.ccg.chart', 'nltk.ccg.lexicon', 'nltk.ccg.logic', 'nltk.help', 'nltk.misc', 'nltk.misc.chomsky', 'nltk.misc.wordfinder', 'nltk.misc.minimalset', 'nltk.misc.babelfish', 'nltk.wsd', 'nltk.treetransforms', 'deeppavlov.core.common.prints', 'deeppavlov.models.tokenizers', 'deeppavlov.models.tokenizers.nltk_tokenizer', 'deeppavlov.models.embedders', 'deeppavlov.models.embedders.fasttext_embedder', 'overrides', 'overrides.overrides', 'overrides.final', 'fastText', 'fastText.FastText', 'fasttext_pybind', 'deeppavlov.models.embedders.abstract_embedder', 'deeppavlov.models.preprocessors', 'deeppavlov.models.preprocessors.one_hotter', 'deeppavlov.models.classifiers', 'deeppavlov.models.classifiers.keras_classification_model', 'keras', 'keras.utils', 'keras.utils.np_utils', 'keras.utils.generic_utils', 'keras.utils.data_utils', 'keras.utils.io_utils', 'h5py', 'h5py._errors', 'h5py._hl', 'h5py._hl.compat', 'h5py._conv', 'h5py.h5r', '_cython_0_28_3', 'h5py._objects', 'h5py.defs', 'h5py.h5t', 'h5py.utils', 'h5py.h5', 'h5py.h5py_warnings', 'h5py.h5z', 'h5py.h5a', 'h5py.h5s', 'h5py.h5p', 'h5py.h5ac', 'h5py._proxy', 'h5py.h5d', 'h5py.h5ds', 'h5py.h5f', 'h5py.h5g', 'h5py.h5i', 'h5py.h5fd', 'h5py._hl.filters', 'h5py._hl.base', 'h5py._hl.files', 'h5py._hl.group', 'h5py.h5o', 'h5py.h5l', 'h5py._hl.dataset', 'h5py._hl.selections', 'h5py._hl.selections2', 'h5py._hl.datatype', 'h5py.version', 'h5py._hl.attrs', 'keras.utils.conv_utils', 'keras.backend', 'keras.backend.common', 'keras.backend.tensorflow_backend', 'tensorflow', 'tensorflow.python', 'tensorflow.python.pywrap_tensorflow', 'tensorflow.python.platform', 'tensorflow.python.platform.self_check', 'tensorflow.python.platform.build_info', 'tensorflow.python.pywrap_tensorflow_internal', 'swig_runtime_data4', '_pywrap_tensorflow_internal', 'tensorflow.core', 'tensorflow.core.framework', 'tensorflow.core.framework.graph_pb2', 'google.protobuf', 'pkg_resources', 'plistlib', 'xml.parsers', 'xml.parsers.expat', 'xml.parsers.expat.model', 'xml.parsers.expat.errors', 'pkg_resources.extern', 'pkg_resources._vendor', 'pkg_resources.extern.six', 'pkg_resources._vendor.six', 'pkg_resources.extern.six.moves', 'pkg_resources._vendor.six.moves', 'pkg_resources.py31compat', 'pkg_resources.extern.appdirs', 'pkg_resources._vendor.packaging.__about__', 'pkg_resources.extern.packaging', 'pkg_resources.extern.packaging.version', 'pkg_resources.extern.packaging._structures', 'pkg_resources.extern.packaging.specifiers', 'pkg_resources.extern.packaging._compat', 'pkg_resources.extern.packaging.requirements', 'pkg_resources.extern.pyparsing', 'pkg_resources.extern.six.moves.urllib', 'pkg_resources.extern.packaging.markers', 'google.protobuf.descriptor', 'google.protobuf.internal', 'google.protobuf.internal.api_implementation', 'google.protobuf.internal._api_implementation', 'google.protobuf.pyext', 'google.protobuf.internal.containers', 'google.protobuf.internal.enum_type_wrapper', 'google.protobuf.message', 'google.protobuf.pyext._message', 'google.protobuf.reflection', 'google.protobuf.pyext.cpp_message', 'google.protobuf.symbol_database', 'google.protobuf.descriptor_pool', 'google.protobuf.descriptor_database', 'google.protobuf.text_encoding', 'google.protobuf.message_factory', 'tensorflow.core.framework.node_def_pb2', 'tensorflow.core.framework.attr_value_pb2', 'tensorflow.core.framework.tensor_pb2', 'tensorflow.core.framework.resource_handle_pb2', 'google.protobuf.internal.well_known_types', 'tensorflow.core.framework.tensor_shape_pb2', 'tensorflow.core.framework.types_pb2', 'tensorflow.core.framework.function_pb2', 'tensorflow.core.framework.op_def_pb2', 'tensorflow.core.framework.versions_pb2', 'tensorflow.core.framework.summary_pb2', 'tensorflow.core.protobuf', 'tensorflow.core.protobuf.meta_graph_pb2', 'google.protobuf.any_pb2', 'tensorflow.core.protobuf.saver_pb2', 'tensorflow.core.protobuf.config_pb2', 'tensorflow.core.framework.cost_graph_pb2', 'tensorflow.core.framework.step_stats_pb2', 'tensorflow.core.framework.allocation_description_pb2', 'tensorflow.core.framework.tensor_description_pb2', 'tensorflow.core.protobuf.debug_pb2', 'tensorflow.core.protobuf.cluster_pb2', 'tensorflow.core.protobuf.rewriter_config_pb2', 'tensorflow.core.protobuf.tensorflow_server_pb2', 'tensorflow.core.util', 'tensorflow.core.util.event_pb2', 'tensorflow.python.framework', 'tensorflow.python.framework.framework_lib', 'tensorflow.python.framework.device', 'tensorflow.python.util', 'tensorflow.python.util.tf_export', 'tensorflow.python.util.tf_decorator', 'tensorflow.python.framework.ops', 'tensorflow.python.eager', 'tensorflow.python.eager.context', 'tensorflow.python.framework.c_api_util', 'tensorflow.core.framework.api_def_pb2', 'tensorflow.python.util.compat', 'tensorflow.python.util.tf_contextlib', 'tensorflow.python.util.is_in_graph_mode', 'tensorflow.python.eager.core', 'tensorflow.python.framework.errors', 'tensorflow.python.framework.errors_impl', 'tensorflow.core.lib', 'tensorflow.core.lib.core', 'tensorflow.core.lib.core.error_codes_pb2', 'tensorflow.python.eager.tape', 'tensorflow.python.framework.cpp_shape_inference_pb2', 'tensorflow.python.framework.dtypes', 'tensorflow.python.framework.op_def_registry', 'tensorflow.python.framework.registry', 'tensorflow.python.platform.tf_logging', 'tensorflow.python.util.tf_stack', 'tensorflow.python.framework.tensor_shape', 'tensorflow.python.framework.traceable_stack', 'tensorflow.python.framework.versions', 'tensorflow.python.ops', 'tensorflow.python.ops.control_flow_util', 'tensorflow.python.platform.app', 'tensorflow.python.platform.flags', 'absl', 'absl.flags', 'absl.flags._argument_parser', 'absl.flags._helpers', 'absl.flags._defines', 'absl.flags._exceptions', 'absl.flags._flag', 'absl.flags._flagvalues', 'absl.flags._validators', 'tensorflow.python.util.decorator_utils', 'tensorflow.python.util.lock_util', 'tensorflow.python.util.deprecation', 'tensorflow.python.util.tf_inspect', 'tensorflow.python.framework.sparse_tensor', 'tensorflow.python.framework.tensor_util', 'tensorflow.python.framework.fast_tensor_util', 'tensorflow.python.framework.random_seed', 'tensorflow.python.framework.importer', 'tensorflow.python.framework.function', 'tensorflow.python.framework.graph_to_function_def', 'tensorflow.python.ops.array_ops', 'tensorflow.python.framework.common_shapes', 'tensorflow.python.framework.constant_op', 'tensorflow.python.eager.execute', 'google.protobuf.text_format', 'google.protobuf.internal.type_checkers', 'google.protobuf.internal.decoder', 'google.protobuf.internal.encoder', 'google.protobuf.internal.wire_format', 'tensorflow.python.ops.gen_array_ops', 'tensorflow.python.framework.op_def_library', 'tensorflow.python.ops.gen_math_ops', 'tensorflow.python.ops.cond_v2_impl', 'tensorflow.python.ops.gen_functional_ops', 'tensorflow.python.ops.resource_variable_ops', 'tensorflow.core.framework.variable_pb2', 'tensorflow.python.ops.gen_resource_variable_ops', 'tensorflow.python.ops.gen_state_ops', 'tensorflow.python.ops.math_ops', 'tensorflow.python.framework.graph_util', 'tensorflow.python.framework.graph_util_impl', 'tensorflow.python.ops.gen_data_flow_ops', 'tensorflow.python.ops.gen_nn_ops', 'tensorflow.python.ops.gen_sparse_ops', 'tensorflow.python.ops.gen_spectral_ops', 'tensorflow.python.util.nest', 'tensorflow.python.ops.variables', 'tensorflow.python.ops.control_flow_ops', 'tensorflow.core.protobuf.control_flow_pb2', 'tensorflow.python.ops.gen_control_flow_ops', 'tensorflow.python.ops.gen_logging_ops', 'tensorflow.python.ops.tensor_array_ops', 'tensorflow.python.util.tf_should_use', 'tensorflow.python.ops.state_ops', 'tensorflow.python.training', 'tensorflow.python.training.checkpointable', 'tensorflow.python.training.checkpointable.base', 'tensorflow.python.ops.gen_io_ops', 'tensorflow.python.training.saveable_object', 'tensorflow.python.util.serialization', 'tensorflow.python.ops.variable_scope', 'tensorflow.python.ops.init_ops', 'tensorflow.python.ops.linalg_ops_impl', 'tensorflow.python.ops.gen_linalg_ops', 'tensorflow.python.ops.random_ops', 'tensorflow.python.ops.gen_random_ops', 'tensorflow.python.util.function_utils', 'tensorflow.python.framework.load_library', 'tensorflow.python.client', 'tensorflow.python.client.client_lib', 'tensorflow.python.client.session', 'tensorflow.python.ops.session_ops', 'tensorflow.python.ops.standard_ops', 'tensorflow.python.ops.array_grad', 'tensorflow.python.ops.sparse_ops', 'tensorflow.python.ops.check_ops', 'tensorflow.python.ops.cudnn_rnn_grad', 'tensorflow.python.ops.gen_cudnn_rnn_ops', 'tensorflow.python.ops.data_flow_grad', 'tensorflow.python.ops.data_flow_ops', 'tensorflow.python.lib', 'tensorflow.python.lib.io', 'tensorflow.python.lib.io.python_io', 'tensorflow.python.lib.io.tf_record', 'tensorflow.python.ops.manip_grad', 'tensorflow.python.ops.manip_ops', 'tensorflow.python.ops.gen_manip_ops', 'tensorflow.python.ops.math_grad', 'tensorflow.python.ops.random_grad', 'tensorflow.python.ops.sparse_grad', 'tensorflow.python.ops.spectral_grad', 'tensorflow.python.ops.spectral_ops', 'tensorflow.python.ops.state_grad', 'tensorflow.python.ops.tensor_array_grad', 'tensorflow.python.ops.clip_ops', 'tensorflow.python.ops.special_math_ops', 'tensorflow.python.ops.confusion_matrix', 'tensorflow.python.ops.functional_ops', 'tensorflow.python.ops.gradients', 'tensorflow.python.eager.backprop', 'tensorflow.python.eager.imperative_grad', 'tensorflow.python.ops.custom_gradient', 'tensorflow.python.ops.gradients_impl', 'tensorflow.python.ops.control_flow_grad', 'tensorflow.python.ops.image_grad', 'tensorflow.python.ops.gen_image_ops', 'tensorflow.python.ops.linalg_grad', 'tensorflow.python.ops.linalg_ops', 'tensorflow.python.ops.linalg', 'tensorflow.python.ops.linalg.linalg_impl', 'tensorflow.python.ops.logging_ops', 'tensorflow.python.ops.histogram_ops', 'tensorflow.python.ops.io_ops', 'tensorflow.python.ops.lookup_ops', 'tensorflow.python.ops.gen_lookup_ops', 'tensorflow.python.ops.string_ops', 'tensorflow.python.ops.gen_string_ops', 'tensorflow.python.ops.numerics', 'tensorflow.python.ops.parsing_ops', 'tensorflow.python.ops.gen_parsing_ops', 'tensorflow.python.ops.partitioned_variables', 'tensorflow.python.ops.script_ops', 'tensorflow.python.ops.gen_script_ops', 'tensorflow.python.ops.template', 'tensorflow.python.eager.function', 'tensorflow.python.eager.graph_only_ops', 'tensorflow.python.training.checkpointable.util', 'tensorflow.core.protobuf.checkpointable_object_graph_pb2', 'tensorflow.python.training.optimizer', 'tensorflow.python.training.distribute', 'tensorflow.python.data', 'tensorflow.python.data.ops', 'tensorflow.python.data.ops.dataset_ops', 'tensorflow.python.compat', 'tensorflow.python.compat.compat', 'tensorflow.python.data.ops.iterator_ops', 'tensorflow.python.data.util', 'tensorflow.python.data.util.nest', 'tensorflow.python.data.util.sparse', 'tensorflow.python.ops.gen_dataset_ops', 'tensorflow.python.data.util.random_seed', 'tensorflow.python.framework.smart_cond', 'tensorflow.python.data.ops.readers', 'tensorflow.python.data.util.convert', 'tensorflow.python.ops.losses', 'tensorflow.python.ops.losses.losses_impl', 'tensorflow.python.ops.nn', 'tensorflow.python.ops.ctc_ops', 'tensorflow.python.ops.gen_ctc_ops', 'tensorflow.python.ops.nn_grad', 'tensorflow.python.ops.nn_ops', 'tensorflow.python.ops.embedding_ops', 'tensorflow.python.ops.nn_impl', 'tensorflow.python.ops.candidate_sampling_ops', 'tensorflow.python.ops.gen_candidate_sampling_ops', 'tensorflow.python.ops.weights_broadcast_ops', 'tensorflow.python.ops.sets', 'tensorflow.python.ops.sets_impl', 'tensorflow.python.ops.gen_set_ops', 'tensorflow.python.ops.losses.util', 'tensorflow.python.training.device_util', 'tensorflow.python.training.slot_creator', 'tensorflow.python.training.saver', 'tensorflow.python.framework.meta_graph', 'tensorflow.python.framework.graph_io', 'tensorflow.python.lib.io.file_io', 'tensorflow.python.platform.gfile', 'tensorflow.python.training.training_util', 'tensorflow.python.training.checkpoint_state_pb2', 'tensorflow.python.training.checkpointable.data_structures', 'tensorflow.python.training.checkpointable.layer_utils', 'tensorflow.python.training.checkpointable.tracking', 'tensorflow.python.ops.initializers_ns', 'tensorflow.python.keras', 'tensorflow.python.keras.activations', 'tensorflow.python.keras.backend', 'tensorflow.python.ops.image_ops', 'tensorflow.python.ops.image_ops_impl', 'tensorflow.python.keras.utils', 'tensorflow.python.keras.utils.data_utils', 'tensorflow.python.keras.utils.generic_utils', 'tensorflow.python.keras.utils.io_utils', 'tensorflow.python.keras.utils.layer_utils', 'tensorflow.python.keras.utils.conv_utils', 'tensorflow.python.keras.utils.multi_gpu_utils', 'tensorflow.python.keras.engine', 'tensorflow.python.keras.engine.base_layer', 'tensorflow.python.keras.constraints', 'tensorflow.python.keras.initializers', 'tensorflow.python.keras.regularizers', 'tensorflow.python.keras.utils.tf_utils', 'tensorflow.python.keras.engine.input_layer', 'tensorflow.python.keras.engine.training', 'tensorflow.python.keras.losses', 'tensorflow.python.keras.metrics', 'tensorflow.python.keras.optimizers', 'tensorflow.python.keras.engine.training_arrays', 'tensorflow.python.keras.callbacks', 'tensorflow.python.summary', 'tensorflow.python.summary.summary', 'google.protobuf.json_format', 'tensorflow.python.ops.gen_summary_ops', 'tensorflow.python.ops.summary_op_util', 'tensorflow.python.ops.summary_ops', 'tensorflow.python.summary.text_summary', 'tensorflow.python.summary.writer', 'tensorflow.python.summary.writer.writer', 'tensorflow.python.summary.plugin_asset', 'tensorflow.python.summary.writer.event_file_writer', 'tensorflow.python.summary.writer.event_file_writer_v2', 'tensorflow.python.ops.summary_ops_v2', 'tensorflow.python.summary.writer.writer_cache', 'tensorflow.python.keras.engine.training_utils', 'tensorflow.python.keras.engine.training_eager', 'tensorflow.python.keras.engine.training_generator', 'tensorflow.python.keras.engine.network', 'tensorflow.python.keras.engine.saving', 'yaml', 'yaml.error', 'yaml.tokens', 'yaml.events', 'yaml.nodes', 'yaml.loader', 'yaml.reader', 'yaml.scanner', 'yaml.parser', 'yaml.composer', 'yaml.constructor', 'yaml.resolver', 'yaml.dumper', 'yaml.emitter', 'yaml.serializer', 'yaml.representer', 'tensorflow.python.keras.utils.np_utils', 'tensorflow.python.keras.utils.vis_utils', 'tensorflow.python.keras.applications', 'tensorflow.python.keras.applications.densenet', 'tensorflow.python.keras.applications.imagenet_utils', 'tensorflow.python.keras.layers', 'tensorflow.python.keras.layers.advanced_activations', 'tensorflow.python.keras.layers.convolutional', 'tensorflow.python.keras.layers.pooling', 'tensorflow.python.keras.layers.core', 'tensorflow.python.keras.layers.embeddings', 'tensorflow.python.keras.layers.local', 'tensorflow.python.keras.layers.merge', 'tensorflow.python.keras.layers.noise', 'tensorflow.python.keras.layers.normalization', 'tensorflow.python.keras.layers.recurrent', 'tensorflow.python.keras.layers.convolutional_recurrent', 'tensorflow.python.keras.layers.cudnn_recurrent', 'tensorflow.python.keras.layers.wrappers', 'tensorflow.python.keras.layers.serialization', 'tensorflow.python.keras.models', 'tensorflow.python.keras.engine.sequential', 'tensorflow.python.keras.applications.inception_resnet_v2', 'tensorflow.python.keras.applications.inception_v3', 'tensorflow.python.keras.applications.mobilenet', 'tensorflow.python.keras.applications.nasnet', 'tensorflow.python.keras.applications.resnet50', 'tensorflow.python.keras.applications.vgg16', 'tensorflow.python.keras.applications.vgg19', 'tensorflow.python.keras.applications.xception', 'tensorflow.python.keras.datasets', 'tensorflow.python.keras.datasets.boston_housing', 'tensorflow.python.keras.datasets.cifar10', 'tensorflow.python.keras.datasets.cifar', 'tensorflow.python.keras.datasets.cifar100', 'tensorflow.python.keras.datasets.fashion_mnist', 'tensorflow.python.keras.datasets.imdb', 'tensorflow.python.keras.preprocessing', 'tensorflow.python.keras.preprocessing.image', 'scipy.ndimage', 'scipy.ndimage.filters', 'scipy.ndimage._ni_support', 'scipy.ndimage._nd_image', 'scipy.ndimage._ni_docstrings', 'scipy.ndimage.fourier', 'scipy.ndimage.interpolation', 'scipy.ndimage.measurements', '_ni_label', 'scipy.ndimage._ni_label', 'scipy.ndimage.morphology', 'scipy.ndimage.io', 'tensorflow.python.keras.preprocessing.sequence', 'tensorflow.python.keras.preprocessing.text', 'tensorflow.python.keras.datasets.mnist', 'tensorflow.python.keras.datasets.reuters', 'tensorflow.python.keras.estimator', 'tensorflow.python.estimator', 'tensorflow.python.estimator.estimator_lib', 'tensorflow.python.estimator.canned', 'tensorflow.python.estimator.canned.baseline', 'tensorflow.python.estimator.estimator', 'tensorflow.python.estimator.model_fn', 'tensorflow.python.estimator.export', 'tensorflow.python.estimator.export.export_output', 'tensorflow.python.saved_model', 'tensorflow.python.saved_model.signature_def_utils', 'tensorflow.python.saved_model.signature_def_utils_impl', 'tensorflow.python.saved_model.signature_constants', 'tensorflow.python.saved_model.utils', 'tensorflow.python.saved_model.utils_impl', 'tensorflow.python.saved_model.tag_constants', 'tensorflow.python.training.monitored_session', 'tensorflow.python.ops.resources', 'tensorflow.python.training.basic_session_run_hooks', 'tensorflow.python.client.timeline', 'tensorflow.python.training.session_run_hook', 'tensorflow.python.training.summary_io', 'tensorflow.python.summary.summary_iterator', 'tensorflow.python.training.coordinator', 'tensorflow.python.training.queue_runner', 'tensorflow.python.training.queue_runner_impl', 'tensorflow.core.protobuf.queue_runner_pb2', 'tensorflow.python.training.session_manager', 'tensorflow.python.estimator.run_config', 'tensorflow.python.training.server_lib', 'tensorflow.python.util.compat_internal', 'tensorflow.python.estimator.util', 'tensorflow.python.training.training', 'tensorflow.python.ops.sdca_ops', 'tensorflow.python.ops.gen_sdca_ops', 'tensorflow.python.training.adadelta', 'tensorflow.python.training.training_ops', 'tensorflow.python.training.gen_training_ops', 'tensorflow.python.training.adagrad', 'tensorflow.python.training.adagrad_da', 'tensorflow.python.training.proximal_adagrad', 'tensorflow.python.training.adam', 'tensorflow.python.training.ftrl', 'tensorflow.python.training.momentum', 'tensorflow.python.training.moving_averages', 'tensorflow.python.training.rmsprop', 'tensorflow.python.training.gradient_descent', 'tensorflow.python.training.proximal_gradient_descent', 'tensorflow.python.training.sync_replicas_optimizer', 'tensorflow.python.training.input', 'tensorflow.python.layers', 'tensorflow.python.layers.utils', 'tensorflow.python.training.basic_loops', 'tensorflow.python.training.checkpoint_utils', 'tensorflow.python.training.device_setter', 'tensorflow.python.training.supervisor', 'tensorflow.python.training.warm_starting_util', 'tensorflow.python.training.checkpoint_ops', 'tensorflow.python.ops.gen_checkpoint_ops', 'tensorflow.core.example', 'tensorflow.core.example.example_pb2', 'tensorflow.core.example.feature_pb2', 'tensorflow.python.training.learning_rate_decay', 'tensorflow.python.estimator.export.export', 'tensorflow.python.ops.metrics', 'tensorflow.python.ops.metrics_impl', 'tensorflow.python.saved_model.builder', 'tensorflow.python.saved_model.builder_impl', 'tensorflow.core.protobuf.saved_model_pb2', 'tensorflow.python.saved_model.constants', 'tensorflow.python.training.evaluation', 'tensorflow.python.estimator.canned.head', 'tensorflow.python.estimator.canned.metric_keys', 'tensorflow.python.estimator.canned.prediction_keys', 'tensorflow.python.feature_column', 'tensorflow.python.feature_column.feature_column', 'tensorflow.python.layers.base', 'tensorflow.python.ops.losses.losses', 'tensorflow.python.estimator.canned.optimizers', 'tensorflow.python.estimator.canned.boosted_trees', 'tensorflow.python.ops.boosted_trees_ops', 'tensorflow.python.ops.gen_boosted_trees_ops', 'tensorflow.python.estimator.canned.dnn', 'tensorflow.python.layers.core', 'tensorflow.python.layers.normalization', 'tensorflow.python.estimator.canned.dnn_linear_combined', 'tensorflow.python.estimator.canned.linear', 'tensorflow.python.estimator.canned.parsing_utils', 'tensorflow.python.estimator.export.export_lib', 'tensorflow.python.estimator.exporter', 'tensorflow.python.estimator.gc', 'tensorflow.python.estimator.inputs', 'tensorflow.python.estimator.inputs.inputs', 'tensorflow.python.estimator.inputs.numpy_io', 'tensorflow.python.estimator.inputs.queues', 'tensorflow.python.estimator.inputs.queues.feeding_functions', 'tensorflow.python.estimator.inputs.queues.feeding_queue_runner', 'pandas', 'pytz', 'pytz.exceptions', 'pytz.lazy', 'pytz.tzinfo', 'pytz.tzfile', 'pandas.compat', 'pandas.compat.chainmap', 'pandas.compat.numpy', 'pandas._libs', 'pandas._libs.tslib', 'pandas._libs.tslibs', 'pandas._libs.tslibs.conversion', 'pandas._libs.tslibs.np_datetime', 'pandas._libs.tslibs.nattype', 'pandas._libs.tslibs.timedeltas', 'pandas._libs.tslibs.timezones', 'pandas._libs.tslibs.parsing', 'pandas._libs.tslibs.ccalendar', 'pandas._libs.tslibs.strptime', 'pandas._libs.tslibs.timestamps', 'pandas._libs.tslibs.fields', 'pandas._libs.hashtable', 'pandas._libs.missing', 'pandas._libs.lib', 'pandas.core', 'pandas.core.config_init', 'pandas.core.config', 'pandas.io', 'pandas.io.formats', 'pandas.io.formats.printing', 'pandas.core.dtypes', 'pandas.core.dtypes.inference', 'pandas.io.formats.console', 'pandas.io.formats.terminal', 'pandas.core.api', 'pandas.core.algorithms', 'pandas.core.dtypes.cast', 'pandas.core.dtypes.common', 'pandas._libs.algos', 'pandas.core.dtypes.dtypes', 'pandas.core.dtypes.generic', 'pandas.core.dtypes.base', 'pandas.errors', 'pandas.core.dtypes.missing', 'pandas.core.common', 'pandas.util', 'pandas.util._decorators', 'pandas._libs.properties', 'pandas.core.util', 'pandas.core.util.hashing', 'pandas._libs.hashing', 'pandas.core.arrays', 'pandas.core.arrays.base', 'pandas.compat.numpy.function', 'pandas.util._validators', 'pandas.core.arrays.categorical', 'pandas.core.accessor', 'pandas.core.base', 'pandas.core.nanops', 'pandas.core.missing', 'pandas.core.groupby', 'pandas.core.groupby.groupby', 'pandas.core.index', 'pandas.core.indexes', 'pandas.core.indexes.api', 'pandas.core.indexes.base', 'pandas._libs.index', 'pandas._libs.tslibs.period', 'pandas._libs.tslibs.frequencies', 'pandas._libs.tslibs.resolution', 'pandas.tseries', 'pandas.tseries.offsets', 'pandas.core.tools', 'pandas.core.tools.datetimes', 'dateutil.easter', 'pandas._libs.tslibs.offsets', 'pandas.tseries.frequencies', 'pandas._libs.join', 'pandas.core.ops', 'pandas._libs.ops', 'pandas.core.indexes.frozen', 'pandas.core.dtypes.concat', 'pandas.core.sorting', 'pandas.core.strings', 'pandas.core.indexes.category', 'pandas.core.indexes.multi', 'pandas.core.indexes.interval', 'pandas._libs.interval', 'pandas.core.indexes.datetimes', 'pandas.core.indexes.numeric', 'pandas.core.indexes.datetimelike', 'pandas.core.tools.timedeltas', 'pandas.core.indexes.timedeltas', 'pandas.core.indexes.range', 'pandas.core.indexes.period', 'pandas.core.frame', 'pandas.core.generic', 'pandas.core.indexing', 'pandas._libs.indexing', 'pandas.core.internals', 'pandas._libs.internals', 'pandas.core.sparse', 'pandas.core.sparse.array', 'pandas._libs.sparse', 'pandas.io.formats.format', 'pandas.io.common', 'pandas.core.series', 'pandas.core.indexes.accessors', 'pandas.plotting', 'pandas.plotting._misc', 'pandas.plotting._style', 'pandas.plotting._compat', 'pandas.plotting._tools', 'pandas.plotting._core', 'pandas.plotting._converter', 'matplotlib', 'matplotlib.cbook', 'matplotlib.cbook.deprecation', 'matplotlib.rcsetup', 'matplotlib.fontconfig_pattern', 'pyparsing', 'matplotlib.colors', 'matplotlib._color_data', 'cycler', 'matplotlib._version']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:33:05.288 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 272: [initializing `KerasClassificationModel` from saved]\n",
      "2018-10-29 16:33:05.649 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 282: [loading weights from model.h5]\n",
      "2018-10-29 16:33:05.840 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 256)      77056       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 256)      153856      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 256)      230656      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 256)      1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 256)      1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 256)      1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 256)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 256)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 256)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7)            28          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 542,675\n",
      "Trainable params: 540,925\n",
      "Non-trainable params: 1,750\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# now one can initialize model\n",
    "m = build_model_from_config(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['GetWeather']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m([\"Is it freezing in Offerman, California?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-10-29 16:33:07.839 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find /home/dilyara/Documents/GitHub/reserve/DeepPavlov/download/snips/valid.csv file\n",
      "2018-10-29 16:33:07.840 WARNING in 'deeppavlov.dataset_readers.basic_classification_reader'['basic_classification_reader'] at line 97: Cannot find /home/dilyara/Documents/GitHub/reserve/DeepPavlov/download/snips/test.csv file\n",
      "2018-10-29 16:33:07.842 INFO in 'deeppavlov.dataset_iterators.basic_classification_iterator'['basic_classification_iterator'] at line 73: Splitting field <<train>> to new fields <<['train', 'valid']>>\n",
      "2018-10-29 16:33:07.847 INFO in 'deeppavlov.core.data.simple_vocab'['simple_vocab'] at line 97: [loading vocabulary from /home/dilyara/Documents/GitHub/reserve/DeepPavlov/download/classifiers/intents_snips_v8/classes.dict]\n",
      "2018-10-29 16:33:07.848 INFO in 'deeppavlov.models.embedders.fasttext_embedder'['fasttext_embedder'] at line 52: [loading fastText embeddings from `/home/dilyara/Documents/GitHub/reserve/DeepPavlov/download/embeddings/wiki.en.bin`]\n",
      "2018-10-29 16:33:28.444 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 272: [initializing `KerasClassificationModel` from saved]\n",
      "2018-10-29 16:33:28.825 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 282: [loading weights from model.h5]\n",
      "2018-10-29 16:33:29.23 INFO in 'deeppavlov.models.classifiers.keras_classification_model'['keras_classification_model'] at line 134: Model was successfully initialized!\n",
      "Model summary:\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 15, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 15, 256)      77056       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 15, 256)      153856      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 15, 256)      230656      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 15, 256)      1024        conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 15, 256)      1024        conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 15, 256)      1024        conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 15, 256)      0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 15, 256)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 15, 256)      0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 256)          0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 256)          0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 256)          0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 768)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          76900       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 100)          0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7)            707         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 7)            28          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 7)            0           batch_normalization_5[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 542,675\n",
      "Trainable params: 540,925\n",
      "Non-trainable params: 1,750\n",
      "__________________________________________________________________________________________________\n",
      "2018-10-29 16:33:29.24 INFO in 'deeppavlov.core.commands.train'['train'] at line 221: Testing the best saved model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"valid\": {\"eval_examples_count\": 1589, \"metrics\": {\"sets_accuracy\": 0.9811, \"f1_macro\": 0.9808, \"roc_auc\": 0.9989}, \"time_spent\": \"0:00:01\", \"examples\": [{\"x\": \"Put some mac wiseman in my latino caliente playlist. \", \"y_predicted\": \"y_pred_labels\", \"y_true\": [\"AddToPlaylist\"]}]}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'valid': OrderedDict([('sets_accuracy', 0.9811),\n",
       "              ('f1_macro', 0.9808),\n",
       "              ('roc_auc', 0.9989)])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or one can evaluate model WITHOUT training\n",
    "train_evaluate_model_from_config(config_path, to_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Deep36_reserve",
   "language": "python",
   "name": "deep36_reserve"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
